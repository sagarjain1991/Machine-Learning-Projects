{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Solutions\n",
    "By Sagar Jain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Data set using the rules defined in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate dataset based on the schema provided in the assignment, also have included bias as part of weights as X0 as suggested in the Linear Regression Notes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate dataset\n",
    "def generate_dataset(m=100):\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    def generate_vector():\n",
    "        X = []\n",
    "        \n",
    "        #add the bias value in the weight vector itself, as used in the Linear Regression Notes.\n",
    "        X.append(1)\n",
    "        \n",
    "        #Assign standard normal values to X1 .. Xk-1. \n",
    "        for i in range(1, 11):\n",
    "            X.append(np.random.standard_normal())\n",
    "        \n",
    "        X11 = X[1] + X[2] + np.random.normal(0, 0.1)\n",
    "        X12 = X[3] + X[4] + np.random.normal(0, 0.1)\n",
    "        X13 = X[4] + X[5] + np.random.normal(0, 0.1)\n",
    "        X14 = (0.1 * X[7]) + np.random.normal(0, 0.1)\n",
    "        X15 = (2 * X[2]) - 10 + np.random.normal(0, 0.1)\n",
    "        \n",
    "        X.append(X11)\n",
    "        X.append(X12)\n",
    "        X.append(X13)\n",
    "        X.append(X14)\n",
    "        X.append(X15)\n",
    "        \n",
    "        #append values of X from 16 till 20\n",
    "        for i in range(16, 21):\n",
    "            X.append(np.random.standard_normal())\n",
    "            \n",
    "        \n",
    "        #compute Y \n",
    "        Y = 10 + sum([ pow(0.6, r+1) * X[r] for r in range(1, 11) ]) + np.random.normal(0, 0.1)\n",
    "        \n",
    "        #append Y to the data frame\n",
    "        X.append(Y)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    for x in range(1, m+1):\n",
    "        data.append(generate_vector())\n",
    "        \n",
    "    #Create header list\n",
    "    headers = ['X'+str(x) for x in range(0, 21)] + ['Y']\n",
    "    \n",
    "    dataframe = pd.DataFrame(data, columns=headers)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.870219</td>\n",
       "      <td>1.015434</td>\n",
       "      <td>0.522406</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>-0.570068</td>\n",
       "      <td>-2.100410</td>\n",
       "      <td>0.186731</td>\n",
       "      <td>0.322923</td>\n",
       "      <td>0.868861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>-0.611443</td>\n",
       "      <td>-0.000716</td>\n",
       "      <td>-7.942622</td>\n",
       "      <td>0.129702</td>\n",
       "      <td>0.417974</td>\n",
       "      <td>1.103614</td>\n",
       "      <td>0.854345</td>\n",
       "      <td>0.648687</td>\n",
       "      <td>10.492354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.276716</td>\n",
       "      <td>0.090066</td>\n",
       "      <td>-0.685802</td>\n",
       "      <td>0.178330</td>\n",
       "      <td>0.340845</td>\n",
       "      <td>2.618737</td>\n",
       "      <td>0.036680</td>\n",
       "      <td>0.057050</td>\n",
       "      <td>0.969929</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476118</td>\n",
       "      <td>0.480631</td>\n",
       "      <td>-0.042128</td>\n",
       "      <td>-10.041456</td>\n",
       "      <td>-0.347287</td>\n",
       "      <td>0.410373</td>\n",
       "      <td>-0.158444</td>\n",
       "      <td>-0.714374</td>\n",
       "      <td>-0.705297</td>\n",
       "      <td>10.194086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.729889</td>\n",
       "      <td>1.282975</td>\n",
       "      <td>2.620998</td>\n",
       "      <td>-0.627954</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>2.225518</td>\n",
       "      <td>1.763625</td>\n",
       "      <td>-1.308189</td>\n",
       "      <td>-0.211782</td>\n",
       "      <td>...</td>\n",
       "      <td>2.051912</td>\n",
       "      <td>-0.607995</td>\n",
       "      <td>0.070529</td>\n",
       "      <td>-7.528294</td>\n",
       "      <td>0.111380</td>\n",
       "      <td>-0.557843</td>\n",
       "      <td>-0.901044</td>\n",
       "      <td>1.248179</td>\n",
       "      <td>0.283902</td>\n",
       "      <td>10.318309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.844001</td>\n",
       "      <td>-1.220131</td>\n",
       "      <td>-0.301316</td>\n",
       "      <td>-0.248937</td>\n",
       "      <td>0.672890</td>\n",
       "      <td>1.488546</td>\n",
       "      <td>1.825777</td>\n",
       "      <td>-0.268736</td>\n",
       "      <td>-1.509893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466746</td>\n",
       "      <td>0.440232</td>\n",
       "      <td>0.239262</td>\n",
       "      <td>-12.583187</td>\n",
       "      <td>-0.269791</td>\n",
       "      <td>0.743027</td>\n",
       "      <td>0.357963</td>\n",
       "      <td>-0.351635</td>\n",
       "      <td>0.271095</td>\n",
       "      <td>10.145179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.056892</td>\n",
       "      <td>-0.631029</td>\n",
       "      <td>1.404445</td>\n",
       "      <td>-0.766170</td>\n",
       "      <td>-0.114534</td>\n",
       "      <td>-0.981898</td>\n",
       "      <td>1.326630</td>\n",
       "      <td>-0.890633</td>\n",
       "      <td>-0.711419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649177</td>\n",
       "      <td>-0.814483</td>\n",
       "      <td>0.212413</td>\n",
       "      <td>-11.161925</td>\n",
       "      <td>0.121936</td>\n",
       "      <td>-0.156732</td>\n",
       "      <td>-1.351449</td>\n",
       "      <td>1.424516</td>\n",
       "      <td>0.431136</td>\n",
       "      <td>10.411994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X0        X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   1  0.870219  1.015434  0.522406 -0.017129 -0.570068 -2.100410  0.186731   \n",
       "1   1  0.276716  0.090066 -0.685802  0.178330  0.340845  2.618737  0.036680   \n",
       "2   1 -0.729889  1.282975  2.620998 -0.627954 -0.000265  2.225518  1.763625   \n",
       "3   1  0.844001 -1.220131 -0.301316 -0.248937  0.672890  1.488546  1.825777   \n",
       "4   1  1.056892 -0.631029  1.404445 -0.766170 -0.114534 -0.981898  1.326630   \n",
       "\n",
       "         X8        X9    ...           X12       X13       X14        X15  \\\n",
       "0  0.322923  0.868861    ...      0.648900 -0.611443 -0.000716  -7.942622   \n",
       "1  0.057050  0.969929    ...     -0.476118  0.480631 -0.042128 -10.041456   \n",
       "2 -1.308189 -0.211782    ...      2.051912 -0.607995  0.070529  -7.528294   \n",
       "3 -0.268736 -1.509893    ...     -0.466746  0.440232  0.239262 -12.583187   \n",
       "4 -0.890633 -0.711419    ...      0.649177 -0.814483  0.212413 -11.161925   \n",
       "\n",
       "        X16       X17       X18       X19       X20          Y  \n",
       "0  0.129702  0.417974  1.103614  0.854345  0.648687  10.492354  \n",
       "1 -0.347287  0.410373 -0.158444 -0.714374 -0.705297  10.194086  \n",
       "2  0.111380 -0.557843 -0.901044  1.248179  0.283902  10.318309  \n",
       "3 -0.269791  0.743027  0.357963 -0.351635  0.271095  10.145179  \n",
       "4  0.121936 -0.156732 -1.351449  1.424516  0.431136  10.411994  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_dataset(m=100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Weights and biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True weight and biases\n",
    "true_weights = [10, 0.36, 0.21599999999999997, 0.1296, 0.07775999999999998, 0.04665599999999999, 0.027993599999999993, 0.016796159999999994, 0.010077695999999997,  0.006046617599999997,  0.0036279705599999985]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to Fit Naive & Ridge Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit naive regression\n",
    "def fit_regression(dataframe, l=0, t='naive'):\n",
    "    \n",
    "    size = len(list(dataframe)) -1\n",
    "    \n",
    "    #split the X and Y from the dataframe\n",
    "    X = dataframe.iloc[:, 0:size]\n",
    "    Y = dataframe.iloc[:,-1]\n",
    "    \n",
    "    #compute sigma\n",
    "    if t == 'naive':\n",
    "        Sigma = np.dot(X.T, X) \n",
    "    elif t == 'ridge':\n",
    "        Sigma = np.add(np.dot(X.T, X), (l * np.identity(size)))\n",
    "    #print(Sigma)\n",
    "    \n",
    "    #compute sigma inverse\n",
    "    try:\n",
    "        Sigma_inverse = np.linalg.inv(Sigma)\n",
    "        #print(Sigma_inverse)\n",
    "    except LinAlgError:\n",
    "        print('Matrix cannot be inversed')\n",
    "        \n",
    "    #compute w hat \n",
    "    type = np.dot(Sigma_inverse, np.dot(X.T, Y))\n",
    "        \n",
    "    return type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.02987  0.40638  0.25452  0.09357  0.01887  0.01999  0.03028  0.01452\n",
      "  0.01012  0.00211  0.00797 -0.04118  0.03721  0.02849  0.01093  0.00272\n",
      "  0.00476 -0.00373 -0.00056  0.00675  0.0048 ]\n"
     ]
    }
   ],
   "source": [
    "naive = fit_regression(df, t='naive')\n",
    "print(naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method to Fit Lasso Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lasso_regression(dataframe, l=0):\n",
    "    \n",
    "    #split the X and Y from the dataframe\n",
    "    size = len(list(dataframe)) -1\n",
    "    X = dataframe.iloc[:, 0:size]\n",
    "    Y = dataframe.iloc[:,-1]\n",
    "    \n",
    "    w = np.zeros(size)\n",
    "    for k in range(100):\n",
    "        for i in range(X.shape[1]):\n",
    "            if (i == 0):\n",
    "                w[i] = w[i] + ((np.sum(Y - np.dot(X, w))) / (X.shape[0]))\n",
    "            else:\n",
    "                temp_1 = (-np.matmul(X.iloc[:, i].T, (Y - (np.dot(X, w)))) + (l / 2))\n",
    "                val_1 = temp_1 / np.matmul(X.iloc[:, i].T, X.iloc[:, i])\n",
    "                val_2 = (-np.matmul(X.iloc[:, i].T, (Y - (np.dot(X, w)))) - l / 2) / np.matmul(X.iloc[:, i].T, X.iloc[:, i])\n",
    "                if (val_1 < w[i]):\n",
    "                    w[i] = w[i] - val_1\n",
    "                elif (w[i] < val_2):\n",
    "                    w[i] = w[i] - val_2\n",
    "                else:\n",
    "                    w[i] = 0\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.01719  0.16814  0.01954  0.07716  0.       0.00823  0.02519  0.0114\n",
      "  0.00486  0.       0.00117  0.19236  0.04794  0.0358   0.       0.00132\n",
      "  0.00165  0.       0.       0.00142  0.     ]\n"
     ]
    }
   ],
   "source": [
    "lasso = fit_lasso_regression(df, l=10)\n",
    "print(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute error\n",
    "def compute_error(dataframe, type):\n",
    "    \n",
    "    err = 0\n",
    "    size = len(list(dataframe)) -1\n",
    "    rows = dataframe.shape[1]\n",
    "    type = np.array(type).reshape((size,1))\n",
    "    \n",
    "    for index in range(0,rows):\n",
    "        row = dataframe.iloc[index:index+1,:]\n",
    "        x = np.array(row.iloc[0][:-1].tolist()).reshape((size,1))\n",
    "        y = row.iloc[0]['Y']\n",
    "        \n",
    "        err = err + (y - np.dot(type.T, x))**2\n",
    "    \n",
    "    err = round(err[0][0]/rows, 5)\n",
    "\n",
    "    return err        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01103"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_error(df, naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Generate a data set of size m = 1000. Solve the naive least squares regression model for the weights and bias that minimize the training error - how do they compare to the true weights and biases? What did your model conclude as the most significant and least significant features - was it able to prune anything? Simulate a large test set of data and estimate the 'true' error of your solved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.468244</td>\n",
       "      <td>-0.107078</td>\n",
       "      <td>-0.869273</td>\n",
       "      <td>-0.500579</td>\n",
       "      <td>-1.482541</td>\n",
       "      <td>0.788338</td>\n",
       "      <td>-0.179473</td>\n",
       "      <td>-1.565799</td>\n",
       "      <td>-2.024198</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.424668</td>\n",
       "      <td>-2.181210</td>\n",
       "      <td>-0.114959</td>\n",
       "      <td>-10.315224</td>\n",
       "      <td>-0.893649</td>\n",
       "      <td>-1.213986</td>\n",
       "      <td>-0.363027</td>\n",
       "      <td>0.250095</td>\n",
       "      <td>1.115611</td>\n",
       "      <td>9.682922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.263009</td>\n",
       "      <td>0.672741</td>\n",
       "      <td>-1.276907</td>\n",
       "      <td>-1.355845</td>\n",
       "      <td>0.382586</td>\n",
       "      <td>1.101637</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>-0.022493</td>\n",
       "      <td>0.113484</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.615758</td>\n",
       "      <td>-1.121970</td>\n",
       "      <td>0.097953</td>\n",
       "      <td>-8.733477</td>\n",
       "      <td>0.836930</td>\n",
       "      <td>0.826950</td>\n",
       "      <td>0.934055</td>\n",
       "      <td>-0.641918</td>\n",
       "      <td>0.393064</td>\n",
       "      <td>10.117723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.680507</td>\n",
       "      <td>0.206036</td>\n",
       "      <td>0.615020</td>\n",
       "      <td>0.084483</td>\n",
       "      <td>-0.846453</td>\n",
       "      <td>0.952930</td>\n",
       "      <td>-0.818257</td>\n",
       "      <td>0.190081</td>\n",
       "      <td>2.136686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849165</td>\n",
       "      <td>-0.637284</td>\n",
       "      <td>-0.039821</td>\n",
       "      <td>-9.715435</td>\n",
       "      <td>1.635666</td>\n",
       "      <td>0.491069</td>\n",
       "      <td>-0.746536</td>\n",
       "      <td>0.618499</td>\n",
       "      <td>-0.000203</td>\n",
       "      <td>10.425596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.008258</td>\n",
       "      <td>-0.981953</td>\n",
       "      <td>-0.742750</td>\n",
       "      <td>-0.505203</td>\n",
       "      <td>-0.275252</td>\n",
       "      <td>-0.808217</td>\n",
       "      <td>-0.510242</td>\n",
       "      <td>-0.636793</td>\n",
       "      <td>-0.967854</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.200666</td>\n",
       "      <td>-0.747700</td>\n",
       "      <td>-0.092345</td>\n",
       "      <td>-11.970017</td>\n",
       "      <td>-0.839604</td>\n",
       "      <td>0.690257</td>\n",
       "      <td>-1.284233</td>\n",
       "      <td>-1.603481</td>\n",
       "      <td>-1.370369</td>\n",
       "      <td>9.424147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.302371</td>\n",
       "      <td>1.654045</td>\n",
       "      <td>0.773009</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>1.854057</td>\n",
       "      <td>0.910489</td>\n",
       "      <td>1.073731</td>\n",
       "      <td>0.768216</td>\n",
       "      <td>-0.067239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831438</td>\n",
       "      <td>1.789426</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>-6.750360</td>\n",
       "      <td>0.566120</td>\n",
       "      <td>-0.251164</td>\n",
       "      <td>-0.465668</td>\n",
       "      <td>-1.682377</td>\n",
       "      <td>0.363020</td>\n",
       "      <td>11.139206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X0        X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   1 -0.468244 -0.107078 -0.869273 -0.500579 -1.482541  0.788338 -0.179473   \n",
       "1   1  0.263009  0.672741 -1.276907 -1.355845  0.382586  1.101637  0.741350   \n",
       "2   1  0.680507  0.206036  0.615020  0.084483 -0.846453  0.952930 -0.818257   \n",
       "3   1 -0.008258 -0.981953 -0.742750 -0.505203 -0.275252 -0.808217 -0.510242   \n",
       "4   1  1.302371  1.654045  0.773009  0.013460  1.854057  0.910489  1.073731   \n",
       "\n",
       "         X8        X9    ...           X12       X13       X14        X15  \\\n",
       "0 -1.565799 -2.024198    ...     -1.424668 -2.181210 -0.114959 -10.315224   \n",
       "1 -0.022493  0.113484    ...     -2.615758 -1.121970  0.097953  -8.733477   \n",
       "2  0.190081  2.136686    ...      0.849165 -0.637284 -0.039821  -9.715435   \n",
       "3 -0.636793 -0.967854    ...     -1.200666 -0.747700 -0.092345 -11.970017   \n",
       "4  0.768216 -0.067239    ...      0.831438  1.789426 -0.003377  -6.750360   \n",
       "\n",
       "        X16       X17       X18       X19       X20          Y  \n",
       "0 -0.893649 -1.213986 -0.363027  0.250095  1.115611   9.682922  \n",
       "1  0.836930  0.826950  0.934055 -0.641918  0.393064  10.117723  \n",
       "2  1.635666  0.491069 -0.746536  0.618499 -0.000203  10.425596  \n",
       "3 -0.839604  0.690257 -1.284233 -1.603481 -1.370369   9.424147  \n",
       "4  0.566120 -0.251164 -0.465668 -1.682377  0.363020  11.139206  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_dataset(m=1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the Model and compute Training Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [10.02987  0.40638  0.25452  0.09357  0.01887  0.01999  0.03028  0.01452\n",
      "  0.01012  0.00211  0.00797 -0.04118  0.03721  0.02849  0.01093  0.00272\n",
      "  0.00476 -0.00373 -0.00056  0.00675  0.0048 ]\n",
      "\n",
      "True Weights:\n",
      " [10, 0.36, 0.216, 0.1296, 0.07776, 0.04666, 0.02799, 0.0168, 0.01008, 0.00605, 0.00363]\n",
      "\n",
      "Difference between True Bias and Weights with the Trained Bias and Weights: \n",
      " [0.02987, 0.04638, 0.03852, -0.03603, -0.05889, -0.02667, 0.00229, -0.00228, 4e-05, -0.00394, 0.00434]\n"
     ]
    }
   ],
   "source": [
    "naive = fit_regression(df, t='naive')\n",
    "print('Weights:\\n',naive)\n",
    "\n",
    "#compare the weights with true weights and bias.\n",
    "trained_weight = naive[0:11]\n",
    "print('\\nTrue Weights:\\n',[round(x,5) for x in true_weights])\n",
    "difference = [round(trained_weight[x] - true_weights[x],5) for x in range(0, len(true_weights))]\n",
    "print('\\nDifference between True Bias and Weights with the Trained Bias and Weights: \\n',difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the Trained weights(and bias) and True Weights(and bias) we see that there is a small difference between the two, as expected.\n",
    "\n",
    "Based on the value of the weights we can see which features are significant and which are not. If the value of weights are very small that concludes that those features are of lesser value. So from our output we see that the bias, and weights from  X1 to X5 are most significant and X16 to X20 are the least significant and we can prune the least significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:\n",
      " 0.00802\n"
     ]
    }
   ],
   "source": [
    "train_err = compute_error(df, naive)\n",
    "print('Training Error:\\n',train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute 'True' Error based on dataset of 10000 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = generate_dataset(m=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Error:\n",
      " 0.0067\n"
     ]
    }
   ],
   "source": [
    "print('True Error:\\n', compute_error(test_data, naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Write a program to take a data set of size m and a parameter $\\lambda$, and solve for the ridge regression model for that data. Write another program to take the solved model and estimate the true error by evaluating that model on a large test data set. For data sets of size m = 1000, plot estimated true error of the ridge regression model as a function of $\\lambda$. What is the optimal $\\lambda$ to minimize testing error? What are the weights and biases ridge regression gives at this $\\lambda$, and how do they compare to the true weights? What did your model conclude as the most significant and least significant features - was it able to prune anything? How does the optimal ridge regression model compare to the naive least squares model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the Ridge Regression model to the data, take $\\lambda = 10$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [ 0.48805 -0.25608  1.48261  0.08545  0.02194  0.02966  0.02166  0.0177\n",
      "  0.01584 -0.0031   0.00817  0.62208  0.04285  0.01699  0.00675 -0.95094\n",
      "  0.00977 -0.0012  -0.00478  0.00357  0.00558]\n"
     ]
    }
   ],
   "source": [
    "ridge = fit_regression(df, l=10, t='ridge')\n",
    "print('Weights:\\n',ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute True Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Error:\n",
      " 0.01119\n"
     ]
    }
   ],
   "source": [
    "print('True Error:\\n',compute_error(test_data, ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8HPWd//HXR91yt1ZyxbisC3KhCVOkEEy1aYaEy5HyCEm4cJeES+Fo6UBy+QVygSMHlxxJ7kIaPQFDMC0YEts0Y1zBxnIBy5JtuduyLVnS5/fHjsxaSJas1Wq02vfz8dBDszPfmf3sYPatme/Md8zdERER6aiMsAsQEZHUpiAREZGEKEhERCQhChIREUmIgkRERBKiIBERkYQoSEREJCEKEum2zGy9mZ2bhO2eZWYVR9H+c2Y2r7PrEOkpFCQiITOz58zs/KNof1RBKJJsChKREJlZb+Bk4OVO3m5WZ24v0fcOsx5JPgWJpAQzm2Zmr5jZTjOrMrN7zCwnbrmb2ZfNbLWZ7TGzH5jZ2GCd3Wb2cHz7YJ1vmdnW4BTap+PmF5jZ7GC914Gxzda728w2BMvfNLOPtFLzaWa2ycwy4+ZdbmZL45qdA8x399rgMy4MtrvZzO5sYZu9gTnAMDPbG/wMM7NbzOxRM/u9me0GPmdmvzGzH8ate9iRTLDeY2ZWbWbrzOyrR9j/uWb2H2b2flDbL8ysV/x2zewmM9sE/F9L81rbtqQ+BYmkigbgG0AEOJ3YF/CXm7WZQeyv+9OAG4H7gE8DxwCTgU/GtR0SbGs4cBVwn5lNCJbdCxwAhgJfCH7ivQGcAAwC/gg8YmZ5zQt291eBGuDsuNmfCtZpciHwl2D6buBud+9HLLwebmGbNcBMoNLd+wQ/lcHiWcCjwADgD83XjWdmGcCTwJJgH5wDfN3MLmhllduB8cHnjgbrfC9u+RBi++NY4JojzJMeSEEiKcHd33T3V9293t3XA/8DfLRZs9vdfbe7rwCWA8+5+1p330Xsr/gTm7X/rrvXuvvLxL7MPxEcPXwc+J6717j7cuD+ZrX83t23BbX8FMgFJtCyBwgCzMz6EguOB+KWzwSeDqYPAlEzi7j73iCIjsYr7v64uze6+/422p4CFLr7be5e5+5rgV8CVzZvaGYGfBH4hrtvd/c9wI+atW0Evh/sz/1HmCc9kIJEUoKZjTezp4JTRbuJfZFFmjXbHDe9v4XXfeJe7wj+um/yHjAMKASygA3NlsXX8m9m9o6Z7TKznUD/Fmpp8kfgY2aWC3wMWOTu7wXbmQLsdvem97qa2F/9K83sDTO7uJVttmZD200OOZbY6bGdTT/At4DBLbQtBPKBN+PaPhPMb1Lt7geardfSPOmBFCSSKn4OrATGBad+vgVYAtsbGPQ3NBkJVALVQD2x02HxywAI+kNuAj4BDHT3AcCu1mpx97eJBdFMjnxaC3df7e6fBIqInUp6tFmNh5q28pmaz68hFgBNhsRNbwDWufuAuJ++7n5hC9vdSiyIJ8W17e/u8cHcUk16RkWaUJBIqugL7Ab2mtlE4EudsM1bzSwnCIeLgUfcvQH4E3CLmeWbWTGxPpT4OuqJBU6WmX0P6NfG+/wR+CpwJvBI3PyL+OC0Fmb2GTMrdPdGYGcwu6GF7W0GCsysfxvvuxi40MwGmdkQ4Otxy14Hdged4b3MLNPMJpvZKc03EtTzS+AuMysKah1+hP4USTMKEkkV1xP7i34PsS+1hxLc3iZgB7GjkD8A/+LuK4Nl1xI7DbYJ+A2HX3H0LLH+lneJHWkcoO1TSg8AZwEvuvtWgCAEjgMWxLWbAawws73EOt6vbOnUUFDnA8Da4FTTsFbe93fEOtPXA88Rt8+CwLyEWOf5OmJHHb8idpquJTcB5cCrwanFF2i9X0jSjOkJiSJdz8w+AVzh7p8IuxaRROmIRCQcO4G7wi5CpDPoiERERBKiIxIREUlIWox/E4lEfNSoUWGXISKSUt58882t7l7YVru0CJJRo0axcOHCsMsQEUkpZvZe2610aktERBKkIBERkYQoSEREJCEKEhERSYiCREREEqIgERGRhChIREQkIQqSI/jdK+t5amllm+1ERNJZUoPEzGaY2SozKzezm1tYnmtmDwXLXzOzUcH8AjOba2Z7zeyeuPb5ZvYXM1tpZivM7MfJrP+RNyv47YJ23Y8jIpK2khYkwbOv7yX2ZLhi4JPBQ4LiXU3skadRYiOh3h7MPwB8l9gzKJr7D3efSOz526VmNjMZ9QOURiMsen8HNbX1yXoLEZGUl8wjkmlAubuvdfc64EFgVrM2s4D7g+lHgXPMzNy9xt3nEQuUQ9x9n7vPDabrgEXAiGR9gLJohPpG5/V125P1FiIiKS+ZQTKcw58cVxHMa7GNu9cTe/Z1QXs2bmYDiD3h7a8JV9qKk48dSG5WBvPKtybrLUREUl4yg8RamNf84SftafPhDZtlEXvU6M/cfW0rba4xs4VmtrC6urrNYluSl53JKaMGMV9BIiLSqmQGSQVwTNzrEcSej91imyAc+gPtOY90H7Da3f+ztQbufp+7l7h7SWFhm6Mgt6o0GmHlpj1s2fOhR2eLiAjJDZI3gHFmNtrMcoArgdnN2swGrgqmrwBe9DYe2WhmPyQWOF/v5HpbVBaNAPDKmm1d8XYiIiknaUES9HlcCzwLvAM87O4rzOw2M7s0aPZroMDMyoHrgEOXCJvZeuBO4HNmVmFmxWY2Avg2savAFpnZYjP7p2R9BoDiYf0YkJ/NvNU6vSUi0pKkPtjK3Z8Gnm4273tx0weAf2hl3VGtbLalfpWkycwwzhhbwPzyrbg7Zl369iIi3Z7ubG+H0miEyl0HWLe1JuxSRES6HQVJOzT1k+jqLRGRD1OQtMPIQfmMGNhL95OIiLRAQdIOZkZZNMKCNdtoaGzzNhcRkbSiIGmn0miEPQfqWbZxV9iliIh0KwqSdjpjbGzkFvWTiIgcTkHSTgV9cike2k/3k4iINKMgOQpl4yK8+d4O9tc1hF2KiEi3oSA5CqXRCHUNjbyxXsPKi4g0UZAchVNGDSQnM0P9JCIicRQkRyE/J4sTRw7Q/SQiInEUJEepLBphReVuttfUhV2KiEi3oCA5SqXjNKy8iEg8BclRmjq8P31zs3R6S0QkoCA5SlmZGZwWDCsvIiIKkg4pi0Z4f/s+3t+2L+xSRERCpyDpgNKmYeXX6KhERERB0gFjC3szpF+e+klERFCQdIiZURqNsKB8K40aVl5E0pyCpIPKxhWwY99B3q7aHXYpIiKhUpB0UOlYPX5XRAQUJB1W1C+P8YP7qJ9ERNKegiQBpdEIb6zfzoGDGlZeRNKXgiQBZdEIBw42suj9HWGXIiISGgVJAk4dU0BmhqmfRETSmoIkAX1yszjxmAHMK9cAjiKSvhQkCSqNRlhWsZNd+w6GXYqISCgUJAkqGxeh0eGVtToqEZH0pCBJ0AnHDKB3Tqb6SUQkbSlIEpSdmcGpYzSsvIikLwVJJyiNRli7tYaNO/eHXYqISJdTkHSCsqiGSxGR9KUg6QTjB/ch0idXQSIiaUlB0gnMjLJorJ/EXcPKi0h6SWqQmNkMM1tlZuVmdnMLy3PN7KFg+WtmNiqYX2Bmc81sr5nd02ydfzezDWa2N5m1H63SaISte+tYtXlP2KWIiHSppAWJmWUC9wIzgWLgk2ZW3KzZ1cAOd48CdwG3B/MPAN8Frm9h008C05JSdAKaHr87b7VOb4lIeknmEck0oNzd17p7HfAgMKtZm1nA/cH0o8A5ZmbuXuPu84gFymHc/VV3r0pi3R0ybEAvxhT2Vj+JiKSdZAbJcGBD3OuKYF6Lbdy9HtgFFHTGm5vZNWa20MwWVldXd8Ym21QWjfDauu3U1Td2yfuJiHQHyQwSa2Fe857o9rTpEHe/z91L3L2ksLCwMzbZptJohH11DSzesLNL3k9EpDtIZpBUAMfEvR4BVLbWxsyygP7A9iTWlFSnjSkgw9BTE0UkrSQzSN4AxpnZaDPLAa4EZjdrMxu4Kpi+AnjRU/j62f69spk6YoD6SUQkrSQtSII+j2uBZ4F3gIfdfYWZ3WZmlwbNfg0UmFk5cB1w6BJhM1sP3Al8zswqmq74MrM7zKwCyA/m35Ksz9ARpdECFm/YyZ4DGlZeRNKDpfABQLuVlJT4woULu+S9FqzZyqd++Rq/vqqEc44b3CXvKSKSDGb2pruXtNVOd7Z3spNGDiQvO0P9JCKSNhQknSwvO5NTRg1SP4mIpA0FSRKURSO8u3kvW3Z/6H5KEZEeR0GSBE3Dpcxfo6MSEen5FCRJUDy0HwPzs5m3Ws9xF5GeT0GSBBkZxhnRiIaVF5G0oCBJkrJohE27D7CmuibsUkREkkpBkiR6/K6IpAsFSZIcMyifkYPydT+JiPR4CpIkKo1GeHXNNuobNKy8iPRcCpIkKotG2FNbz9KNu8IuRUQkaRQkSXT62ALMYL4evysiPZiCJIkG9c5h0rB+6icRkR5NQZJkpdEIi97fwb66+rBLERFJCgVJkpVFIxxscF5fl7IPfhQROSIFSZKdMmoQOVkZup9ERHosBUmS5WVnUnLsQOaVa9wtEemZFCRdoDQa4Z2q3WzdWxt2KSIinU5B0gWahktZsEZHJSLS8yhIusDk4f3pl5el+0lEpEdSkHSBzAzjjLER5mlYeRHpgRQkXaR0XISNO/fz3rZ9YZciItKpFCRdpKmfRHe5i0hPoyDpIqMK8hk+oJfuJxGRHkdB0kXMjNJoAQvWbKOhUf0kItJzKEi6UGk0wq79B1lRqWHlRaTnUJB0oTPGqp9ERHoeBUkXKuyby8QhfdVPIiI9yhGDxMwyzOzUriomHZRFI7yxfgcHDjaEXYqISKc4YpC4eyNwdxfVkhZKoxHq6htZuH5H2KWIiHSK9pzaet7MZiW9kjQxbfQgsjKM+Wt0ektEeoasdrS5FuhvZrXAfsAAd/dBSa2sh+qdm8VJIweqn0REeoy2+kgMOB7IBvoAhUAk+C0dVBqNsGzjLnbuqwu7FBGRhLXVR+LAn929oflPF9XXI5WNK8AdXtGw8iLSA7Snj+R1MzupIxs3sxlmtsrMys3s5haW55rZQ8Hy18xsVDC/wMzmmtleM7un2Tonm9myYJ2fBUdNKWXqiAH0yc3S/SQi0iO0J0jKiIXJKjNbZGZvmdmitlYys0zgXmAmUAx80syKmzW7Gtjh7lHgLuD2YP4B4LvA9S1s+ufANcC44GdGOz5Dt5KdmcFpYwapn0REeoT2dLZf1sFtTwPK3X0tgJk9CMwC3o5rMwu4JZh+FLjHzMzda4B5ZhaN36CZDQX6ufsrwevfBvXN6WCNoSmNRnjhnS1s2L6PYwblh12OiEiHtXpEYmYfBXD3NcBBd1/T9ANMbse2hwMb4l5XBPNabOPu9cAuoKCNbVa0sc2m+q8xs4VmtrC6urod5XatDx6/q6MSEUltRzq1dVfc9OPNln2/Hdtuqe+i+bC37WnTofbufp+7l7h7SWFh97vILFrUh6K+ucwrV4e7iKS2IwWJtTLd0uuWVADHxL0eAVS21sbMsoD+wPY2tjmijW2mBDOjLBphQflWGjWsvIiksCMFibcy3dLrlrwBjDOz0WaWA1wJzG7WZjZwVTB9BfCiH+Gh5u5eBewxs9OCq7U+CzzRjlq6pdJohG01dazctCfsUkREOuxIne1jzOxPxI4+mqYJXo9ua8PuXm9m1wLPApnA/7r7CjO7DVjo7rOBXwO/M7NyYkciVzatb2brgX5AjpldBpzv7m8DXwJ+A/Qi1smech3tTUqDfpL55VspHtYv5GpERDrGWjsAMLNzjrSiu/81KRUlQUlJiS9cuDDsMlp07p0vM3xAL+7/wrSwSxEROYyZvenuJW21a/WIJJWCIpWVRSM89MYGausbyM3KDLscEZGjpgdbhaw0GmH/wQbeen9n2KWIiHSIgiRkp44ZRGaG6S53EUlZ7Q4SM8tNZiHpql9eNseP6K9xt0QkZbUZJGY2zcyWAauD18eb2X8lvbI0UhaNsGTDTnYfOBh2KSIiR609RyQ/Ay4GtgG4+xJgejKLSjel0QiNDq9qWHkRSUHtCZIMd3+v2Tw9j6QTnThyIL2yM9VPIiIpqT2j/24ws2mAB0PD/yvwbnLLSi85WRmcOmaQ+klEJCW154jkS8B1wEhgM3BaME86UVk0wprqGqp27Q+7FBGRo9JmkLj7Fne/0t0jwc+V7q4/nTvZB8OlqJ9ERFJLm6e2zOyXtDBIo7tfk5SK0tSEwX2J9MlhfvlWrjh5RNsriIh0E+3pI3khbjoPuJzDH1glnSAjwzhjbIR55Vtxd1LwUfQikqbaDBJ3fyj+tZn9Dng+aRWlsbJohNlLKlm9ZS/jB/cNuxwRkXbpyBApo4FjO7sQgdJxsX6SeavVBSUiqaM9d7bvMLPtwc9OYkcj30p+aeln+IBejI701v0kIpJSjnhqK3gK4fHAxmBW45GeYCiJK40W8OdFGznY0Eh2psbUFJHu74jfVEFo/NndG4IfhUiSlY6NUFPXwJINGlZeRFJDe/7kfd3MTkp6JQLA6WMLMNP9JCKSOloNEjNrOu1VRixMVpnZIjN7y8wWdU156WdAfg5ThvdXP4mIpIwj9ZG8DpwEXNZFtUigNBrhl39bS01tPb1z23Orj4hIeI50assA3H1NSz9dVF9aKotGqG90Xl+3PexSRETadKQ/dwvN7LrWFrr7nUmoR4CTjx1IblYG88q3Mn1iUdjliIgc0ZGCJBPoQ3BkIl0nLzuTU0YNUj+JiKSEIwVJlbvf1mWVyGFKoxFuf2YlW/YcoKhvXtjliIi0qs0+EglHWTCs/Ct6/K6IdHNHCpJzuqwK+ZDiYf0YkJ+tcbdEpNtrNUjcXZcMhSgzwzhjbAHzg2HlRUS6Kw3m1I2VRiNU7jrAuq01YZciItIqBUk3Vnbo8bs6vSUi3ZeCpBsbOSifEQN7MU9BIiLdmIKkGzMzyqIRFqzZRkOj+klEpHtSkHRzpdEIew7Us2zjrrBLERFpkYKkmzsjGFb+8bc2tt1YRCQESQ0SM5sRDD9fbmY3t7A818weCpa/Zmaj4pZ9M5i/yswuiJv/NTNbbmYrzOzryay/Oyjok8unpo3kt6+sZ7EediUi3VDSgsTMMoF7gZlAMfBJMytu1uxqYIe7R4G7gNuDdYuBK4FJwAzgv80s08wmA18EphF7BPDFZjYuWZ+hu7h55kQG98vjxkeXUFvfEHY5IiKHSeYRyTSg3N3Xunsd8CAwq1mbWcD9wfSjwDnBc+JnAQ+6e627rwPKg+0dB7zq7vvcvR54Gbg8iZ+hW+ibl82PLp/Cu5v38t9zNYK/iHQvyQyS4cCGuNcVwbwW2wTBsAsoOMK6y4EzzazAzPKBC4FjWnpzM7vGzBaa2cLq6upO+Djhmj6xiMtPHM69c8tZuWl32OWIiBySzCBpadDH5tewttamxfnu/g6x01/PA88AS4D6lt7c3e9z9xJ3LyksLGx/1d3Y9y4upn+vbG58dCn1DY1hlyMiAiQ3SCo4/GhhBFDZWpvgGfH9ge1HWtfdf+3uJ7n7mUHb1Umpvhsa2DuHW2dNYmnFLv53/rqwyxERAZIbJG8A48xstJnlEOs8n92szWzgqmD6CuBFj41QOBu4MriqazQwjtgz5DGzouD3SOBjwANJ/AzdzkVThnJ+8WB++ty7rNcYXCLSDSQtSII+j2uBZ4F3gIfdfYWZ3WZmlwbNfg0UmFk5cB1wc7DuCuBh4G1ip7C+4u5Nlys9ZmZvA08G83ck6zN0R2bGDy6bTE5WBjc9tpRG3fEuIiGzdBiivKSkxBcuXBh2GZ3q4Tc2cONjS/nhZZP5zGnHhl2OiPRAZvamu5e01U53tqeofygZQVk0wo/nrKRy5/6wyxGRNKYgSVFmxv/72BQaGp1v/3mZHn4lIqFRkKSwYwblc8MFE5i7qponFje/IE5EpGsoSFLcVWeM4qSRA7j1yRVs3VsbdjkikoYUJCkuM8O444qp1NQ2cMvsFWGXIyJpSEHSA0SL+vLVc6I8tbSK51ZsCrscEUkzCpIe4p8/OpbjhvbjO48vZ9f+g2GXIyJpREHSQ2RnZvCTK6ayraaOH/3lnbDLEZE0oiDpQSYP7881Z47hoYUbmLd6a9jliEiaUJD0MF87ZxxjIr25+U9L2VfX4sDIIiKdSkHSw+RlZ3L7FVOp2LGfnzy7KuxyRCQNKEh6oFNGDeKzpx/Lbxas5833toddjoj0cAqSHurGGRMZ1r8XNz66lAMH9Zx3kXSyv66Bp5dVceuTK7pk+CQFSQ/VJzeLH31sCmuqa7jnxfKwyxGRJKutb+C5FZv46gNvcfIPn+fLf1jEk0sq2bq3LunvnZX0d5DQfHR8IR8/aQS/eHkNM6cMYdKw/mGXJCKdqK6+kfnlW3lyaSXPr9jMntp6BuRnM+uEYVw8dRinjh5EVmbyjxcUJD3cdy8+jpffreamx5by+JdLu+QflYgkT31DI6+s3cZTS6p4ZsUmdu0/SN+8LC6YPISLpw6lNBohu4v/P1eQ9HAD8nP4waxJfOkPi/jl39fxpbPGhl2SiBylhkbnjfXbeWppJXOWbWJbTR29czI5r3gwF08dxkfGR8jNygytPgVJGpg5ZSgzJw/hrhfe5fxJgxlb2CfskkSkDY2NzlsbdvDkkiqeXlbFlj215GVncM7EwVw8dSjTJxaRlx1eeMRTkKSJW2dNYsGabdz82FIeuuZ0MjIs7JJEpBl3Z2nFLp5aWslfllZRuesAOVkZnDW+kIuPH8Y5E4vondv9vra7X0WSFEV98/juxcVc/8gSfv/ae3z29FFhlyQixMLj7ardPLW0ir8sreL97fvIzjQ+Mq6Q6y+YwHnFg+mblx12mUekIEkjHz9pOLOXVHL7nJWcPbGIEQPzwy5JJG2t3ryHJ5dW8dTSStZW15CZYZwxtoBrp0e5YNIQ+ud37/CIpyBJI2bGjy6fzAV3/Y1v/Xk593/+FMx0ikukq6zbWsNTSyp5amkVqzbvwQxOG13A1WWjmTFpCAV9csMusUMUJGlmxMB8bpo5ke89sYLHFm3kipNHhF2SSI+2Yfs+ngqOPFZU7gbglFEDufXSScycMoSivnkhV5g4BUka+sypx/Lkkkp+8NTbnDk+0iP+IYt0J1W79vOXpVU8ubSKJRt2AnD8MQP4zkXHceGUoQwb0CvkCjuXgiQNZWQYP/74VGbe/Xe+/8QKfv6Zk8MuSSTl7dxXx1+WVfHE4kpeXxcbLHXSsH7cNGMiF08dyjGDem6fpIIkTY0t7MPXzx3HHc+sYs6yKmZOGRp2SSIpZ39dA8+/s5nZizfy8rvVHGxwxhT25rrzxnPx1KGMSZN7thQkaeyaj4zh6WVVfPeJFZw+toAB+TlhlyTS7R1saGRe+VZmL67k2RWb2FfXwJB+eXy+dDSXHj+MScP6pd1FLAqSNJaVmcHtH5/KrHvm84On3uGnnzg+7JJEuiV3Z9H7O3hicexGwW01dfTLy2LWCcO49PjhTBs9iMw0vslXQZLmJg3rz798dCz3zC3n0hOG8dHxhWGXJNJtvLt5D4+/tZHZSyqp2LGf3KwMzi0ezKzjh/HRCYWhjm/VnShIhGvPjjJneRXf+tMynv3GmfTphkMwiHSVjTv3M3txJU8s3sjKTXvIMCgbV8g3zh3P+ZO6/13mYdA3hpCXnckdVxzPFb9YwB3PrOS2WZPDLkmkS22vqePpZVXMXlzJ6+tjV1ydOHIAt1xSzEVTh1HYNzVvFOwqChIB4ORjB/K5M0bxf/PXc/HUYUwbPSjskkSSal9dPc+/vZnZiyt5+d1q6hudaFEfrj9/PJceP5yRBT33ct3OpiCRQ64/fwLPv72Zmx9bytNf+0i3GaJapLMcbGhk3uqtPL54I8+t2Mz+gw0M7Z/H1WWjufSEYRQPTb8rrjqDgkQO6Z2bxY8/NpXP/Po17v7ram6aMTHskkQS1tgYd8XVsiq219TRv1c2l504nFknDGPaqEF6rEKCkhokZjYDuBvIBH7l7j9utjwX+C1wMrAN+Ed3Xx8s+yZwNdAAfNXdnw3mfwP4J8CBZcDn3f1AMj9HOikbF+ETJSO4729ruWjKUCYP13PeJTWt2rSHxxdvZPbiSjbu3E9edgbnHjeYWScM58yQnyjY0yQtSMwsE7gXOA+oAN4ws9nu/nZcs6uBHe4eNbMrgduBfzSzYuBKYBIwDHjBzMYDQ4CvAsXuvt/MHg7a/SZZnyMdffuiYl5aVc0Njy5l9rWlXf78Z5GOqtixj9lLKpm9uJKVm/aQmWGURSP82/njOX/SEF2RmCTJ3KvTgHJ3XwtgZg8Cs4D4IJkF3BJMPwrcY7ETlLOAB929FlhnZuXB9t4Pau5lZgeBfKAyiZ8hLfXvlc0PLpvMP//uTf7n5TVce/a4sEsSadX72/YxZ3kVTy/fdGiAxJNGDuDWSydx0dShRFJ0aPZUkswgGQ5siHtdAZzaWht3rzezXUBBMP/VZusOd/dXzOw/iAXKfuA5d3+upTc3s2uAawBGjhyZ+KdJMxdMGsJFU4fys7+Wc8GkIYwb3DfskkQOWVu9lznLN/H0sqpDQ7NPGd6fGy6YwCVTh+mKqy6WzCBpqffK29mmxflmNpDY0cpoYCfwiJl9xt1//6HG7vcB9wGUlJQ0f19ph1sumcT88q3c9NhSHvmXM9J6CAgJ3+rNe3h62SbmLK9i5aY9QOxej29feBwzJg/p0aPrdnfJDJIK4Ji41yP48GmopjYVZpYF9Ae2H2Hdc4F17l4NYGZ/As4APhQkkrjCvrl8/5JivvHQEu5fsJ4vlI0OuyRJI+7OO1V7mLO8ijnLN1G+ZS9mUHLsQL53cTEzJg/pcc/1SFXJDJI3gHFmNhrYSKxT/FPN2swGrgJeAa4AXnR3N7PZwB/N7E5ine3jgNeBRuA0M8sndmrrHGBhEj9D2rvshOHMXlzJT55dxXnFg/VXnySVu7N8426eXl7FnGVVrN+2jwyDU0cXcNXpx3LBpCEU9dOD2LqbpAVJ0OdxLfAssct//9fdV5jZbcBCd5+A3PrjAAALtElEQVQN/Br4XdCZvp1Y2BC0e5hYx3w98BV3bwBeM7NHgUXB/LcITl9JcpgZ/375FM6/629880/L+N3V03TDlnSqxkZnccVOngn6PCp27CczwzhjbAHXnDmW8ycNVod5N2fuPb/7oKSkxBcu1IFLIn7/6nt85/HlnDZmEN+ceRzHHzMg7JIkhTU2Om++v4Onl1XxzPJNVO06QHZm7FLdmVOGct5xgxnYW8/HCZuZvenuJW2100XV0i6fPnUk7s5/vrCaWffO56IpQ7nhggmMivQOuzRJEQ2NzuvrtjNneSw8tuypJScrgzPHFXLDBRM457jB9O+lkXVTkY5I5KjsOXCQX/59Hb/6+1rq6hv51Kkj+dezx2l0VGnRwYZGXl27jaeXbeK5FZvYVlNHXnYG0ycUMXPKUM6eWKSbBLux9h6RKEikQ7bsOcDP/rqaB17fQF5WBl88cwxf/MgYeutLIe3V1Tcyf81W5iyr4rm3N7Nz30HyczI5e2IRF04ZylkTCsnP0b+TVKAgiaMgSZ611Xv5ybOrmLN8E5E+OXztnHFcOW2khlVJMwcONvD31bHweP6dzew5UE/f3CzOLR7MzMlDOHN8oUaTTkEKkjgKkuRb9P4OfjxnJa+v287oSG9uuGACMycP0RVePVjlzv3MXbWFuSurWbBmK/vqGujfK5vzigdz4ZQhlEY1MGKqU5DEUZB0DXfnxZVbuP2Zlby7eS/HHzOAb86cyGljCsIuTTrBwYZGFr23g7mrqnlp1ZZDd5cPH9CLsycWcW7xYM4YW6Cj0R5EQRJHQdK1GhqdxxZVcNfz71K16wBnTyzixhkTmDikX9ilyVGq3lPLy+9WM3flFv62upo9B+rJyjBOGTWI6RMLOXtiEWML++jIs4dSkMRRkITjwMEGfrNgPffOLWdvbT0fP2kE1503XsNadGMNjc7Sip2HjjqWVuwCoKhvLtMnFDF9YiGl0Qh983SZbjpQkMRRkIRr57467p1bzv0L3gODz58xii+fFaV/vr6MuoOd++r42+qtvLRyCy+9W832mjoyDE4cOZDpEwo5a0IRk4bpEbTpSEESR0HSPVTs2Medz7/Ln9/aSL+8bL4yfSyfPX2UrubpYu7O21W7eWlV7JTVovd30OgwMD+bj44vZPrEIs4cV6g7y0VBEk9B0r28XbmbO55dyUurqhnWP4/rzp/A5ScO1zD1SbS3tp55q7fy0qotzF21hc27awGYPLwfZ08o4qyJRRw/YoD+G8hhFCRxFCTd04I1W/nxnJUsrdjFxCF9uWnGRM6aUKhTKJ3A3VlTvZe5K6uZu2oLb6zfzsEGp29uFh8ZH+GsCUWcNb5QI+nKESlI4ihIuq/GRufp5VX85NlVvLdtnwaFTMD+ugZeXbstdm/Hqi1s2L4fgPGD+zB9QhFnTSiiZNRAXZ4r7aYgiaMg6f7q6ht58I33ufuF1WyrqdOgkK042NBITW09e2vrqaltYG9tPSsqd/Hiyi28smYbtfWN5GVnUDo2wvSJRZw1oZARA/UMGekYBUkcBUnq2Ftbz31/W9tjBoWsb2ikpq6Bmtr6DwVATW09NXWx1x8si83bW9t8nVi7uobGFt/n2IL84PLcIk4dPUgXMEinUJDEUZCknrAGhWxs9ENf7h98gQdf5nFf+rF5QQDU1X9oXlMgHDjY8hd/cxkGvXOy6J2bRe/cTPrkNk1nBdOZsdc5TfMyDy0fVdCb0TpykyRQkMRRkKSutgaFdHf21TV06K/9pnn76hoOm24PC77483Piv/RbCICcuBBoCoCc+ICI/c7LztBFBtLtKEjiKEhSX/ygkAW9c8jKtFhI1NXT3n/C+TmZh76446db+4Lv/aF5HwRFr+xMMnSprPRwekKi9CgnjRzIQ9ecxtxVW3hySRU5mRkfOsXTFBAtnRbKz8nSPRIiSaIgkZRhZpw9cTBnTxwcdikiEkcXlIuISEIUJCIikhAFiYiIJERBIiIiCVGQiIhIQhQkIiKSEAWJiIgkREEiIiIJSYshUsysGnivg6tHgK2dWE6q0/74gPbF4bQ/PtBT9sWx7l7YVqO0CJJEmNnC9ow1ky60Pz6gfXE47Y8PpNu+0KktERFJiIJEREQSoiBp231hF9DNaH98QPvicNofH0irfaE+EhERSYiOSEREJCEKEhERSYiCBDCzQWb2vJmtDn4PbKXdVUGb1WZ2VQvLZ5vZ8uRXnDyJ7Aszyzezv5jZSjNbYWY/7trqO4+ZzTCzVWZWbmY3t7A818weCpa/Zmaj4pZ9M5i/yswu6Mq6k6Gj+8LMzjOzN81sWfD77K6uPRkS+bcRLB9pZnvN7Pquqjnp3D3tf4A7gJuD6ZuB21toMwhYG/weGEwPjFv+MeCPwPKwP09Y+wLIB6YHbXKAvwMzw/5MHdgHmcAaYEzwOZYAxc3afBn4RTB9JfBQMF0ctM8FRgfbyQz7M4W0L04EhgXTk4GNYX+eMPdH3PLHgEeA68P+PJ31oyOSmFnA/cH0/cBlLbS5AHje3be7+w7geWAGgJn1Aa4DftgFtSZbh/eFu+9z97kA7l4HLAJGdEHNnW0aUO7ua4PP8SCx/RIvfj89CpxjZhbMf9Dda919HVAebC9VdXhfuPtb7l4ZzF8B5JlZbpdUnTyJ/NvAzC4j9ofXii6qt0soSGIGu3sVQPC7qIU2w4ENca8rgnkAPwB+CuxLZpFdJNF9AYCZDQAuAf6apDqTqc3PF9/G3euBXUBBO9dNJYnsi3gfB95y99ok1dlVOrw/zKw3cBNwaxfU2aWywi6gq5jZC8CQFhZ9u72baGGem9kJQNTdv9H8XGh3lax9Ebf9LOAB4GfuvvboKwzdET9fG23as24qSWRfxBaaTQJuB87vxLrCksj+uBW4y933BgcoPUbaBIm7n9vaMjPbbGZD3b3KzIYCW1poVgGcFfd6BPAScDpwspmtJ7Y/i8zsJXc/i24qifuiyX3Aanf/z04oNwwVwDFxr0cAla20qQiCsz+wvZ3rppJE9gVmNgL4M/BZd1+T/HKTLpH9cSpwhZndAQwAGs3sgLvfk/yykyzsTpru8AP8hMM7mO9ooc0gYB2xTuWBwfSgZm1Gkfqd7QntC2L9RI8BGWF/lgT2QRax89ij+aBDdVKzNl/h8A7Vh4PpSRze2b6W1O5sT2RfDAjafzzsz9Ed9kezNrfQgzrbQy+gO/wQO5/7V2B18LvpS7EE+FVcuy8Q6zwtBz7fwnZ6QpB0eF8Q++vMgXeAxcHPP4X9mTq4Hy4E3iV2hc63g3m3AZcG03nErrwpB14HxsSt++1gvVWk4FVrnbUvgO8ANXH/FhYDRWF/njD/bcRto0cFiYZIERGRhOiqLRERSYiCREREEqIgERGRhChIREQkIQoSERFJiIJE5CiZ2d4kbHO9mUXCeG+RRClIREQkIWkzRIpIMpnZJcRuwMsBtgGfdvfNZnYLsbughwLjiY0SfRowE9gIXOLuB4PN3GBm04PpT7l7uZmNJvZ4gizgmbj36wM8QWxkgWzgO+7+RHI/pUjLdEQi0jnmAae5+4nEhha/MW7ZWOAiYsOL/x6Y6+5TgP3B/Ca73X0acA/QNE7Z3cDP3f0UYFNc2wPA5e5+EjAd+Kn1tJEAJWUoSEQ6xwjgWTNbBtxAbMytJnOCo45lxB6M1HRksYzYsDpNHoj7fXowXRo3/3dxbQ34kZktBV4gNnT54E75JCJHSUEi0jn+C7gnONL4Z2LjLTWpBXD3RuCgfzAuUSOHn172dkw3+TRQCJzs7icAm5u9p0iXUZCIdI7+xPo8AK7q4Db+Me73K8H0fGIjyEIsPOLfb4u7Hwz6VY7t4HuKJEyd7SJHL9/MKuJe30lsNNdHzGwj8CqxDvajlWtmrxH7A++TwbyvAX80s68RG56/yR+AJ81sIbFRdVd24P1EOoVG/xURkYTo1JaIiCREQSIiIglRkIiISEIUJCIikhAFiYiIJERBIiIiCVGQiIhIQv4/5fRILIgGQKgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = generate_dataset(m=1000)\n",
    "lambda_list = [-0.04, -0.03, -0.02, -0.01, 0.01, 0.02, 0.03, 0.04, 0.05] \n",
    "output = {}\n",
    "\n",
    "for l in lambda_list:\n",
    "    output[l] = compute_error(df, fit_regression(df, l=l, t='ridge'))\n",
    "    \n",
    "plot_list = sorted(output.items())\n",
    "x,y = zip(*plot_list)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('lambda v/s true err')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('True Err')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights and biases when $\\lambda = 0.02$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here bias is at index 0 and rest are the weights\n",
      "Weights:\n",
      " [ 8.4502   0.34592  0.51137  0.12437  0.03386  0.00406  0.02532  0.01727\n",
      "  0.01075  0.01221  0.00466  0.01358  0.00318  0.04086 -0.01177 -0.15491\n",
      " -0.00167  0.00348 -0.0029  -0.00096 -0.00095]\n",
      "\n",
      "True Weights:\n",
      " [10, 0.36, 0.216, 0.1296, 0.07776, 0.04666, 0.02799, 0.0168, 0.01008, 0.00605, 0.00363]\n",
      "\n",
      "Difference between True Bias and Weights with the Trained Bias and Weights: \n",
      " [-1.5498, -0.01408, 0.29537, -0.00523, -0.0439, -0.04259, -0.00267, 0.00047, 0.00067, 0.00616, 0.00103]\n"
     ]
    }
   ],
   "source": [
    "print('Here bias is at index 0 and rest are the weights')\n",
    "ridge = fit_regression(df, l=0.02, t='ridge')\n",
    "print('Weights:\\n',ridge)\n",
    "\n",
    "#compare the weights with true weights and bias.\n",
    "trained_weight = ridge[0:11]\n",
    "print('\\nTrue Weights:\\n',[round(x,5) for x in true_wights])\n",
    "difference = [round(trained_weight[x] - true_wights[x],5) for x in range(0, len(true_wights))]\n",
    "print('\\nDifference between True Bias and Weights with the Trained Bias and Weights: \\n',difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the Trained weights(and bias) and True Weights(and bias) we see that there is a small difference between the two, as expected.\n",
    "\n",
    "Based on the value of the weights we can see which features are significant and which are not. If the value of weights are very small that concludes that those features are of lesser value. So from our output we see that the bias, and weights from  X1 to X5 are most significant and X17 to X20 are the least significant and we can prune the least significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Error:\n",
      " 0.00715\n"
     ]
    }
   ],
   "source": [
    "print('True Error:\\n',compute_error(test_data, ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "How does the optimal ridge regression model compare to the naive least squares model?\n",
    "\n",
    "We can see this by the True error, as the True error for Ridge Regression is less than Naive Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Write a program to take a data set of size m and a parameter $\\lambda$, and solve for the Lasso regression model for that data. For a data set of size m = 1000, show that as $\\lambda$ increases, features are effectively eliminated from the model until all weights are set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.773603</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>0.037082</td>\n",
       "      <td>-1.053734</td>\n",
       "      <td>0.644026</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.367024</td>\n",
       "      <td>-0.373473</td>\n",
       "      <td>0.114640</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.071340</td>\n",
       "      <td>-0.574870</td>\n",
       "      <td>0.184516</td>\n",
       "      <td>-10.097121</td>\n",
       "      <td>1.667048</td>\n",
       "      <td>0.626579</td>\n",
       "      <td>-1.188360</td>\n",
       "      <td>0.048674</td>\n",
       "      <td>0.280283</td>\n",
       "      <td>9.603530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.322008</td>\n",
       "      <td>-1.612480</td>\n",
       "      <td>0.429062</td>\n",
       "      <td>-0.973586</td>\n",
       "      <td>0.953963</td>\n",
       "      <td>-1.787101</td>\n",
       "      <td>1.123238</td>\n",
       "      <td>-1.232796</td>\n",
       "      <td>-1.681746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.706466</td>\n",
       "      <td>-0.101278</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>-13.237014</td>\n",
       "      <td>-0.033773</td>\n",
       "      <td>-0.362333</td>\n",
       "      <td>-0.379620</td>\n",
       "      <td>0.542999</td>\n",
       "      <td>-1.136398</td>\n",
       "      <td>9.426222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.144021</td>\n",
       "      <td>-0.739667</td>\n",
       "      <td>0.985590</td>\n",
       "      <td>-0.442746</td>\n",
       "      <td>0.609574</td>\n",
       "      <td>0.707026</td>\n",
       "      <td>-0.463267</td>\n",
       "      <td>2.352182</td>\n",
       "      <td>-0.737974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705369</td>\n",
       "      <td>0.065483</td>\n",
       "      <td>-0.138413</td>\n",
       "      <td>-11.389494</td>\n",
       "      <td>0.863326</td>\n",
       "      <td>0.570690</td>\n",
       "      <td>1.618593</td>\n",
       "      <td>-0.233146</td>\n",
       "      <td>-1.274820</td>\n",
       "      <td>10.032731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.865826</td>\n",
       "      <td>1.869681</td>\n",
       "      <td>-1.540936</td>\n",
       "      <td>1.081130</td>\n",
       "      <td>-2.650913</td>\n",
       "      <td>-0.104953</td>\n",
       "      <td>-1.789587</td>\n",
       "      <td>-1.756356</td>\n",
       "      <td>1.418613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511343</td>\n",
       "      <td>-1.674901</td>\n",
       "      <td>-0.244582</td>\n",
       "      <td>-6.278297</td>\n",
       "      <td>-0.721765</td>\n",
       "      <td>-0.872394</td>\n",
       "      <td>-0.749263</td>\n",
       "      <td>0.737667</td>\n",
       "      <td>-0.061784</td>\n",
       "      <td>9.768495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.086247</td>\n",
       "      <td>-0.473199</td>\n",
       "      <td>0.923353</td>\n",
       "      <td>-1.031145</td>\n",
       "      <td>-0.679329</td>\n",
       "      <td>-0.094868</td>\n",
       "      <td>0.675072</td>\n",
       "      <td>-0.352337</td>\n",
       "      <td>-1.720380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137442</td>\n",
       "      <td>-1.718172</td>\n",
       "      <td>0.122241</td>\n",
       "      <td>-11.017426</td>\n",
       "      <td>-0.170356</td>\n",
       "      <td>-0.786658</td>\n",
       "      <td>0.730567</td>\n",
       "      <td>-1.320049</td>\n",
       "      <td>-0.824328</td>\n",
       "      <td>10.228620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X0        X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   1 -0.773603 -0.091234  0.037082 -1.053734  0.644026  0.622207  0.367024   \n",
       "1   1 -0.322008 -1.612480  0.429062 -0.973586  0.953963 -1.787101  1.123238   \n",
       "2   1  0.144021 -0.739667  0.985590 -0.442746  0.609574  0.707026 -0.463267   \n",
       "3   1 -0.865826  1.869681 -1.540936  1.081130 -2.650913 -0.104953 -1.789587   \n",
       "4   1  1.086247 -0.473199  0.923353 -1.031145 -0.679329 -0.094868  0.675072   \n",
       "\n",
       "         X8        X9    ...           X12       X13       X14        X15  \\\n",
       "0 -0.373473  0.114640    ...     -1.071340 -0.574870  0.184516 -10.097121   \n",
       "1 -1.232796 -1.681746    ...     -0.706466 -0.101278 -0.040887 -13.237014   \n",
       "2  2.352182 -0.737974    ...      0.705369  0.065483 -0.138413 -11.389494   \n",
       "3 -1.756356  1.418613    ...     -0.511343 -1.674901 -0.244582  -6.278297   \n",
       "4 -0.352337 -1.720380    ...     -0.137442 -1.718172  0.122241 -11.017426   \n",
       "\n",
       "        X16       X17       X18       X19       X20          Y  \n",
       "0  1.667048  0.626579 -1.188360  0.048674  0.280283   9.603530  \n",
       "1 -0.033773 -0.362333 -0.379620  0.542999 -1.136398   9.426222  \n",
       "2  0.863326  0.570690  1.618593 -0.233146 -1.274820  10.032731  \n",
       "3 -0.721765 -0.872394 -0.749263  0.737667 -0.061784   9.768495  \n",
       "4 -0.170356 -0.786658  0.730567 -1.320049 -0.824328  10.228620  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate data\n",
    "df = generate_dataset(m=1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [ 9.99113  0.3221   0.18355  0.10455  0.04953  0.05107  0.02807  0.01874\n",
      "  0.01065  0.00257  0.00576  0.03612  0.02844  0.00077  0.      -0.001\n",
      " -0.00432  0.       0.       0.       0.     ]\n"
     ]
    }
   ],
   "source": [
    "#Fit lasso regression \n",
    "lasso = fit_lasso_regression(df, l=1)\n",
    "print('Weights:\\n',lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For lambda:  1\n",
      "\n",
      "Weights:\n",
      " [ 9.99113  0.3221   0.18355  0.10455  0.04953  0.05107  0.02807  0.01874\n",
      "  0.01065  0.00257  0.00576  0.03612  0.02844  0.00077  0.      -0.001\n",
      " -0.00432  0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  5\n",
      "\n",
      "Weights:\n",
      " [10.00524  0.24299  0.10174  0.07303  0.       0.0283   0.02593  0.01674\n",
      "  0.00823  0.00003  0.00358  0.1136   0.05744  0.02147  0.       0.00042\n",
      " -0.00241  0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  10\n",
      "\n",
      "Weights:\n",
      " [10.01065  0.14415  0.00207  0.07122  0.       0.02449  0.02337  0.01452\n",
      "  0.00544  0.       0.00083  0.21036  0.05651  0.02266  0.       0.00097\n",
      " -0.00001  0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  20\n",
      "\n",
      "Weights:\n",
      " [10.00125  0.1314   0.       0.06518  0.       0.0199   0.01797  0.00949\n",
      "  0.00021  0.       0.       0.21602  0.05762  0.02171  0.       0.\n",
      "  0.       0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  30\n",
      "\n",
      "Weights:\n",
      " [10.00122  0.12572  0.       0.0603   0.       0.01464  0.01274  0.00426\n",
      "  0.       0.       0.       0.21624  0.0576   0.02167  0.       0.\n",
      "  0.       0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  50\n",
      "\n",
      "Weights:\n",
      " [10.001    0.11436  0.       0.04982  0.       0.00468  0.00215  0.\n",
      "  0.       0.       0.       0.21706  0.05825  0.02096  0.       0.\n",
      "  0.       0.       0.       0.       0.     ]\n",
      "\n",
      "\n",
      "For lambda:  100\n",
      "\n",
      "Weights:\n",
      " [9.99951 0.08821 0.      0.00492 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.21825 0.07951 0.      0.      0.      0.      0.\n",
      " 0.      0.      0.     ]\n",
      "\n",
      "\n",
      "For lambda:  200\n",
      "\n",
      "Weights:\n",
      " [9.99771 0.03962 0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.21813 0.0593  0.      0.      0.      0.      0.\n",
      " 0.      0.      0.     ]\n",
      "\n",
      "\n",
      "For lambda:  500\n",
      "\n",
      "Weights:\n",
      " [9.99073 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.16428 0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.     ]\n",
      "\n",
      "\n",
      "For lambda:  10000\n",
      "\n",
      "Weights:\n",
      " [9.97921 0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "lambda_list = [1, 5, 10, 20, 30, 50, 100, 200, 500, 10000]\n",
    "for l in lambda_list:\n",
    "    lasso = fit_lasso_regression(df, l=l)\n",
    "    print('\\n\\nFor lambda: ', l)\n",
    "    print('\\nWeights:\\n', lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We can conlude from above results that, as the value of $\\lambda$ increases the values of weights are all set to zero and all the features are eliminated from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "For data sets of size m = 1000, plot estimated true error of the lasso regression model as a function of $\\lambda$. What is the optimal $\\lambda$ to minimize testing error? What are the weights and biases lasso regression gives at this $\\lambda$, and how do they compare to the true weights? What did your model conclude as the most significant and least signficiant features - was it able to prune anything? How does the optimal regression model compare to the naive least squares model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = generate_dataset(m=1000)\n",
    "lambda_list = [-0.05,  0.01, 0.05, 0.1, 0.5, 1, 5,10] \n",
    "output = {}\n",
    "\n",
    "for l in lambda_list:\n",
    "    output[l] = compute_error(df, fit_lasso_regression(df, l=l))\n",
    "    \n",
    "plot_list = sorted(output.items())\n",
    "x,y = zip(*plot_list)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('lambda v/s true err')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('True Err')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here bias is at index 0 and rest are the weights\n",
      "Weights:\n",
      " [ 9.99831  0.35861  0.21098  0.1074   0.0644   0.05542  0.03133  0.01624\n",
      "  0.00981  0.00771  0.00334  0.00049  0.02254 -0.00935 -0.01448 -0.00026\n",
      "  0.00029  0.00262  0.00233 -0.00011 -0.00317]\n",
      "\n",
      "True Weights:\n",
      " [10, 0.36, 0.216, 0.1296, 0.07776, 0.04666, 0.02799, 0.0168, 0.01008, 0.00605, 0.00363]\n",
      "\n",
      "Difference between True Bias and Weights with the Trained Bias and Weights: \n",
      " [-0.00169, -0.00139, -0.00502, -0.0222, -0.01336, 0.00876, 0.00334, -0.00056, -0.00026, 0.00166, -0.00029]\n"
     ]
    }
   ],
   "source": [
    "print('Here bias is at index 0 and rest are the weights')\n",
    "lasso = fit_lasso_regression(df, l=0.02)\n",
    "print('Weights:\\n',lasso)\n",
    "\n",
    "#compare the weights with true weights and bias.\n",
    "trained_weight = lasso[0:11]\n",
    "print('\\nTrue Weights:\\n',[round(x,5) for x in true_wights])\n",
    "difference = [round(trained_weight[x] - true_wights[x],5) for x in range(0, len(true_wights))]\n",
    "print('\\nDifference between True Bias and Weights with the Trained Bias and Weights: \\n',difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the Trained weights(and bias) and True Weights(and bias) we see that there is a small difference between the two, as expected.\n",
    "\n",
    "Based on the value of the weights we can see which features are significant and which are not. If the value of weights are very small that concludes that those features are of lesser value. So from our output we see that the bias, and weights from  X1 to X5 are most significant and X17 to X20 are the least significant and we can prune the least significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Error:\n",
      " 0.00801\n"
     ]
    }
   ],
   "source": [
    "print('True Error:\\n',compute_error(test_data, lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "How does the optimal lasso regression model compare to the naive least squares model?\n",
    "\n",
    "We can see this by the True error, as the True error for Lasso Regression is less than Naive Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Consider using lasso as a means for feature selection: on a data set of size m = 1000, run lasso regression with the optimal regularization constant from the previous problems, and identify the set of relevant features; then run ridge regression to fit a model to only those features. How can you determine a good ridge regression regularization constant to use here? How does the resulting lasso-ridge combination model compare to the naive least squares model? What features does it conclude are significant or relatively insignificant? How do the testing errors of these two models compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above example lets take the features whose coefficient are > 0.01. So we take features = {Bias, X1, X2, X3, X4, X5, X6, X7, X12, X13, X14} and run ridge regression on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.862531</td>\n",
       "      <td>-0.628756</td>\n",
       "      <td>0.994723</td>\n",
       "      <td>1.657353</td>\n",
       "      <td>-0.537293</td>\n",
       "      <td>-0.521495</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>2.756001</td>\n",
       "      <td>1.196699</td>\n",
       "      <td>-0.012577</td>\n",
       "      <td>9.810986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.361648</td>\n",
       "      <td>-2.818744</td>\n",
       "      <td>-0.523411</td>\n",
       "      <td>-0.542179</td>\n",
       "      <td>-0.337131</td>\n",
       "      <td>1.445675</td>\n",
       "      <td>-0.268246</td>\n",
       "      <td>-1.098928</td>\n",
       "      <td>-0.892108</td>\n",
       "      <td>-0.037346</td>\n",
       "      <td>9.461822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.187080</td>\n",
       "      <td>-0.107199</td>\n",
       "      <td>0.868128</td>\n",
       "      <td>-1.363357</td>\n",
       "      <td>1.160069</td>\n",
       "      <td>-3.086694</td>\n",
       "      <td>-0.473303</td>\n",
       "      <td>-0.553986</td>\n",
       "      <td>-0.288514</td>\n",
       "      <td>-0.158961</td>\n",
       "      <td>9.542071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.007852</td>\n",
       "      <td>-0.069427</td>\n",
       "      <td>-0.549166</td>\n",
       "      <td>-0.028312</td>\n",
       "      <td>-1.266432</td>\n",
       "      <td>0.364680</td>\n",
       "      <td>-2.381698</td>\n",
       "      <td>-0.589647</td>\n",
       "      <td>-1.404401</td>\n",
       "      <td>-0.151436</td>\n",
       "      <td>9.302436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.694722</td>\n",
       "      <td>0.492224</td>\n",
       "      <td>0.333639</td>\n",
       "      <td>0.377573</td>\n",
       "      <td>0.350409</td>\n",
       "      <td>0.811197</td>\n",
       "      <td>1.572970</td>\n",
       "      <td>0.757293</td>\n",
       "      <td>0.693721</td>\n",
       "      <td>0.181841</td>\n",
       "      <td>10.063599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X0        X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   1 -0.862531 -0.628756  0.994723  1.657353 -0.537293 -0.521495  0.025745   \n",
       "1   1  0.361648 -2.818744 -0.523411 -0.542179 -0.337131  1.445675 -0.268246   \n",
       "2   1 -1.187080 -0.107199  0.868128 -1.363357  1.160069 -3.086694 -0.473303   \n",
       "3   1 -1.007852 -0.069427 -0.549166 -0.028312 -1.266432  0.364680 -2.381698   \n",
       "4   1 -0.694722  0.492224  0.333639  0.377573  0.350409  0.811197  1.572970   \n",
       "\n",
       "        X12       X13       X14          Y  \n",
       "0  2.756001  1.196699 -0.012577   9.810986  \n",
       "1 -1.098928 -0.892108 -0.037346   9.461822  \n",
       "2 -0.553986 -0.288514 -0.158961   9.542071  \n",
       "3 -0.589647 -1.404401 -0.151436   9.302436  \n",
       "4  0.757293  0.693721  0.181841  10.063599  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_df = df.copy()\n",
    "modified_df = modified_df[['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X12', 'X13', 'X14', 'Y']]\n",
    "modified_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso-ridge error:\n",
      " 0.01038\n"
     ]
    }
   ],
   "source": [
    "#running ridge regression\n",
    "ridge = fit_regression(modified_df, t='ridge', l=0.2)\n",
    "error = compute_error(modified_df, ridge)\n",
    "print('lasso-ridge error:\\n',error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8VfX9x/HXhy3IkC17g2EJBBCt4qw4cVDFVVel2mqt/VVEbRVxVKzV0rpn1bojalwguBcjWA0krDATZtgQyP78/riHek0TEiEnN+P9fDzy4Nxzvufcz5eRN+ecm/Mxd0dERKS81Yp1ASIiUj0pYEREJBQKGBERCYUCRkREQqGAERGRUChgREQkFAoYEREJhQJGqhwzW2lmJ4Zw3GPNLOMnjL/MzL4s7zpEqgsFjEglZWYfmtnPf8L4nxSQImFTwIhUQmbWCBgCfFbOx61Tnsc70PeOZT0SPgWMVGlmNszMvjGzbWa2zsweMrN6UdvdzH5jZkvNbKeZ3Wlm3YN9dpjZa9Hjg31uMbNNwaW4i6LWtzCzxGC/OUD3IvtNMbP0YPs8Mzu6hJqPMLP1ZlY7at3ZZpYcNewE4Ct3zwnmmBQcd4OZPVDMMRsBHwDtzGxX8NXOzCaaWYKZ/dvMdgCXmdm/zOyuqH1/dOYT7PeGmWWa2Qoz+90+fv/rm9n9ZrY6qO0xMzso+rhmdpOZrQeeLW5dSceWqk8BI1VdAXAD0BIYQeQb82+KjBlF5GzgCGA88ARwEdAR6AdcEDW2bXCs9sClwBNm1jvY9jCQDRwKXBF8RZsLHA40B14CXjezBkULdvdZQBZwfNTqC4N99joVeC9YngJMcfcmRELttWKOmQWcAqx194ODr7XB5tFAAtAMeLHovtHMrBbwDvB98HtwAvB7Mzu5hF0mA72CefcI9rktantbIr8fnYFx+1gn1ZACRqo0d5/n7rPcPd/dVwKPAyOLDJvs7jvcPQVYAHzo7svdfTuR//UPKjL+z+6e4+6fEfkmf15wtnEucJu7Z7n7AuC5IrX82903B7X8DagP9KZ4LxMEm5k1JhIoL0dtPwV4P1jOA3qYWUt33xUE1E/xjbu/5e6F7r6nlLFDgVbuPsndc919OfAkMLboQDMz4CrgBnff4u47gXuKjC0Ebg9+P/fsY51UQwoYqdLMrJeZvRtcctpB5BtcyyLDNkQt7ynm9cFRr7cGZwN7rQLaAa2AOkB6kW3RtfyfmS00s+1mtg1oWkwte70EnGNm9YFzgG/dfVVwnP7ADnff+15XEjlLWGRmc83s9BKOWZL00of8V2cil9m27f0CbgHaFDO2FdAQmBc1dlqwfq9Md88usl9x66QaUsBIVfcosAjoGVxCugWwAzjeIcH9jL06AWuBTCCfyGW16G0ABPdbbgLOAw5x92bA9pJqcfdUIgF1Cvu+PIa7L3X3C4DWRC5JJRSp8b9DS5hT0fVZRIJhr7ZRy+nACndvFvXV2N1PLea4m4gEdN+osU3dPTqwi6tJPUJqCAWMVHWNgR3ALjPrA1xTDse8w8zqBaFxOvC6uxcAU4GJZtbQzOKI3KOJriOfSBDVMbPbgCalvM9LwO+AY4DXo9afxg+XxzCzi82slbsXAtuC1QXFHG8D0MLMmpbyvt8Bp5pZczNrC/w+atscYEdwE/4gM6ttZv3MbGjRgwT1PAk8aGatg1rb7+N+jdQwChip6v5I5AxgJ5Fvdq8e4PHWA1uJnLW8CFzt7ouCbdcSuZy2HvgXP/4E1HQi93OWEDkzyab0S1MvA8cCH7v7JoAgHA4Dvo4aNwpIMbNdRG74jy3uElNQ58vA8uCSVbsS3vcFIjfxVwIfEvV7FgTpGURu2q8gcpbyFJHLfcW5CUgDZgWXKGdS8n0nqWFMHS1FKg8zOw8Y4+7nxboWkQOlMxiRymUb8GCsixApDzqDERGRUOgMRkREQlGjnwPUsmVL79KlS6zLEBGpUubNm7fJ3VuVNq5GB0yXLl1ISkqKdRkiIlWKma0qfZQukYmISEgUMCIiEgoFjIiIhEIBIyIioVDAiIhIKBQwIiISCgWMiIiEQgEjIlKDbNudyx3vpLAzOy/096rRP2gpIlKTfDB/HX9+O4Vtu3M5qntLTowrrlFp+VHAiIhUc5k7c7g9cQHvz19Pv/ZNeP6KYcS1K60f3oFTwIiIVFPuzlvfreGOd1LZnVvA+FG9GXd0N+rUrpi7IwoYEZFqaN32Pdz65gI+XrSRwZ2acd+YgfRofXCF1qCAERGpRtydV+amc897C8kvdG47PY5Lj+xC7VpW4bUoYEREqon0LbuZMDWZr9I2M6JbCyafO4BOLRrGrB4FjIhIFVdY6Dz3zUrum7aY2rWMe87uzwXDOmJW8Wct0RQwIiJV2LLMXdyUkEzSqq0c27sV95zdn3bNDop1WYACRkSkSsovKOTJL1bw4MwlHFS3Ng+cN5CzB7WP+VlLNAWMiEgVs2j9Dm58PZn5a7Yzqm9bJp3Vl9aNG8S6rP+hgBERqSJy8wt5+JM0Hvk0jaYH1eWRiwZzav9DY11WiUL9aRszG2Vmi80szcwmFLO9vpm9GmyfbWZdorbdHKxfbGYnR61vZmYJZrbIzBaa2Yhg/eFmNsvMvjOzJDMbFubcREQqUnLGNs586EumfLSU0we0Y8YNIyt1uECIZzBmVht4GDgJyADmmlmiu6dGDbsS2OruPcxsLDAZON/M4oCxQF+gHTDTzHq5ewEwBZjm7mPMrB6w9zN49wF3uPsHZnZq8PrYsOYnIlIRsvMKeHDmEp78fDmtGzfg6UvjOeGwcJ8hVl7CvEQ2DEhz9+UAZvYKMBqIDpjRwMRgOQF4yCJ3qEYDr7h7DrDCzNKAYWaWAhwDXAbg7rlAbrC/A3sfrtMUWBvOtEREKsbclVu4KSGZ5ZuyuGBYR24+9TCaNKgb67LKLMyAaQ+kR73OAIaXNMbd881sO9AiWD+ryL7tgT1AJvCsmQ0E5gHXu3sW8HtgupndT+TS35HFFWVm44BxAJ06dTqQ+YmIhCIrJ5+/Tl/Mc9+spH2zg3jxV8M5qkfLWJf1k4V5D6a4z8p5GceUtL4OMBh41N0HAVnA3ns71wA3uHtH4Abg6eKKcvcn3D3e3eNbtWpV+ixERCrQl0s3cfLfP+e5b1Zy6YguTP/9MVUyXCDcM5gMoGPU6w7872WrvWMyzKwOkUtbW/axbwaQ4e6zg/UJ/BAwlwLXB8uvA0+VzzRERMK3IzuPe95byCtz0+nWshGv/XoEQ7s0j3VZByTMM5i5QE8z6xrcjB8LJBYZk0gkGADGAB+7uwfrxwafMusK9ATmuPt6IN3Megf7nMAP93TWAiOD5eOBpWFMSkSkvH20cAMnPfAZryWl8+uR3Xj/+qOrfLhAiGcwwT2Va4HpQG3gGXdPMbNJQJK7JxK5jPVCcBN/C5EQIhj3GpHwyAd+G3yCDOA64MUgtJYDlwfrrwKmBGdC2QT3WUREKqutWZH2xW99t5bebRrzxCXxDOzYLNZllRuLnDDUTPHx8Z6UlBTrMkSkBnp//jpue3sB23bnce3xPfjNsT2oV6diGoEdKDOb5+7xpY3TT/KLiFSgjTuzue2tFKalrKd/+6a8cOVwDjs0/PbFsaCAERGpAO7O1G/XMOndVPbkFTDhlD786mddK6x9cSwoYEREQrZ22x5ueXM+ny7OJL7zIUweM4DurSq2fXEsKGBEREJSWOi8PHc1f3l/EQWFzsQz4vjliC7UikH74lhQwIiIhGDV5iwmvDGfb5Zv5qgeLbj3nAF0bB679sWxoIARESlHBYXOv75eyV+nL6JurVrce05/zh8a+/bFsaCAEREpJ2kbdzI+IZlvV2/j+D6tufvsfhzatHK0L44FBYyIyAHKLyjk8c+XM2XmUhrWr83fzz+c0Ye3q5FnLdEUMCIiByB17Q7Gv/E9C9bs4NT+bbnjzH60alw/1mVVCgoYEZH9kJNfwMMfp/HIp8to1rAej140mFMqeYfJiqaAERH5ib5L38b4hO9ZsmEX5wxuz22nx9GsYb1Yl1XpKGBERMooO6+AB2Ys4akvltOmSQOevWwox/VpHeuyKi0FjIhIGcxZsYWb3khmxaYsLhzeiZtP6UPjKtS+OBYUMCIi+7ArJ5/7pi3i+W9W0al5Q166ajhHdq+aHSYrmgJGRKQEXyzNZMIb81m7fQ9XHNWVP57ci4b19G2zrPQ7JSJSxPY9edz9XiqvJWXQrVUjEq4ewZDOVb/DZEVTwIiIRJmRuoFb35zP5qxcrjm2O9ef0JMGdWvHuqwqSQEjIgJsycplYmIKid+vpU/bxjx96VD6d2ga67KqNAWMiNRo7s5789dx+9sp7MjO4w8n9eLqkd2rTPviykwBIyI11sYd2fzprQV8mLqBgR2act+YI+jdtnGsy6o2FDAiUuO4OwnzMrjz3VRy8gu55dQ+XHFU9W5fHAsKGBGpUdZs28PNU+fz+ZJMhnVpzr3n9qdbDWhfHAsKGBGpEQoLnRfnrObe9xfiwKTRfbl4eOca0744FhQwIlLtrdyUxU1vJDN7xRaO7tmSe87uX+PaF8eCAkZEqq2CQufZr1Zw/4eLqVu7FvedO4BfxHeo8Y3AKooCRkSqpaUbdnJjQjLfpW/jxMNac9dZ/WnbtEGsy6pRFDAiUq3kFRTy+GfL+MdHaTSqX5spYw/nzIFqXxwLoX4mz8xGmdliM0szswnFbK9vZq8G22ebWZeobTcH6xeb2clR65uZWYKZLTKzhWY2ImrbdcH4FDO7L8y5iUjlk7J2O6Mf+or7P1zCSX3bMOMPIxl9eHuFS4yEdgZjZrWBh4GTgAxgrpkluntq1LArga3u3sPMxgKTgfPNLA4YC/QF2gEzzayXuxcAU4Bp7j7GzOoBDYP3Ow4YDQxw9xwzUxcgkRoiJ7+Af36UxmOfLeOQRvV47OIhjOrXNtZl1XhhXiIbBqS5+3IAM3uFSABEB8xoYGKwnAA8ZJH/aowGXnH3HGCFmaUBw8wsBTgGuAzA3XOB3GD/a4B7g31w943hTU1EKov/rN7K+IRklm7cxZghHfjzaXE0bahGYJVBmJfI2gPpUa8zgnXFjnH3fGA70GIf+3YDMoFnzew/ZvaUmTUKxvQCjg4utX1mZkOLK8rMxplZkpklZWZmHtgMRSRm9uQWcNe7qZz76Ndk5eTzr8uHcv8vBipcKpEwA6a4i55exjElra8DDAYedfdBQBaw995OHeAQ4AjgRuA1K+bCq7s/4e7x7h7fqlWrMk1ERCqXWcs3M2rK5zz15QouHN6J6Tccw7G9dVW8sgnzElkG0DHqdQdgbQljMsysDtAU2LKPfTOADHefHaxP4IeAyQCmursDc8ysEGhJ5IxHRKqBXTn53PvBQv49azWdWzTk5auOYET3FrEuS0oQ5hnMXKCnmXUNbsaPBRKLjEkELg2WxwAfBwGRCIwNPmXWFegJzHH39UC6mfUO9jmBH+7pvAUcD2BmvYB6wKZwpiYiFe2zJZmc/ODnvDh7Nb/6WVemXX+MwqWSC+0Mxt3zzexaYDpQG3jG3VPMbBKQ5O6JwNPAC8FN/C1EQohg3GtEwiMf+G3wCTKA64AXg9BaDlwerH8GeMbMFhC58X9pEFYiUoVt353Hne+lkjAvgx6tDybh6iMZ0vmQWJclZWA1+XtwfHy8JyUlxboMESnBhynrufWtBWzJyuXqkd247ni1L64MzGyeu8eXNk4/yS8ilc7mXTncnpjCu8nrOOzQJjx72VD6tVf74qpGASMilYa7807yOiYmprArO58//rwXvx7ZnbpqBFYlKWBEpFLYsCObW99cwMyFGxjYsRl/HTOAXm3UvrgqU8CISEy5O68nZXDne6nk5hfyp9MO4/KjulJbjcCqPAWMiMRMxtbd3Dx1Pl8s3cTwrs2ZfO4AurRsVPqOUiUoYESkwhUWOv+evYrJHywC4M6z+nHRsE5qX1zNKGBEpEKt2JTFTQnJzFkZaV/8l3P60+EQtS+ujhQwIlIhCgqdp79czt8+XEL9OrW4b8wAfjFE7YurMwWMiIRuSdC++Pv0bZwU14a7zupHmyZqX1zdKWBEJDR5BYU89uky/vHxUho3qMs/LxjE6QMO1VlLDaGAEZFQLFiznRsTklm4bgdnDGzHxDPiaHFw/ViXJRVIASMi5So7r4B/fLSUxz9fTotG9XjikiH8vK/aF9dEChgRKTfzVm1lfML3LMvM4rz4Dtx6qtoX12QKGBE5YLtz87l/+hKe/XoF7ZoexPNXDOOYXuoYW9MpYETkgHy9bBMT3pjP6i27+eWIzowf1YeD6+tbiyhgRGQ/7czO4y8fLOKl2avp0qIhr447guHd1GFSfqCAEZGf7JPFG7ll6nw27MjmqqO78oeTenNQPTUCkx9TwIhImW3bncukd1OZ+u0aerY+mEeuOZJBndS+WIqngBGRMpm2YD1/emsB23bnct3xPbj2+B7Ur6OzFimZAkZE9mnTrhxufzuF9+avo2+7Jjx3xVD6tlP7YimdAkZEiuXuJH6/lomJKWTlFHDjyb0Zd0w3tS+WMlPAiMj/WL89m1vfnM9HizYyqFOkfXGP1mpfLD+NAkZE/svdeXVuOne/t5C8wkL+fHoclx3ZRe2LZb8oYEQEgPQtkfbFX6Zt4ohukfbFnVuofbHsv30GjJnVAoa6++wKqkdEKlhhofP8Nyu5b/piaplx99n9uGCo2hfLgdtnwLh7oZlNAY6ooHpEpAItz9zFTW8kM3flVkb2asU95/SnfbODYl2WVBNl+TjIDDMbvT8HN7NRZrbYzNLMbEIx2+ub2avB9tlm1iVq283B+sVmdnLU+mZmlmBmi8xsoZmNKHLMP5qZm1nL/alZpCbILyjksc+WMWrKFyxev5P7fzGQf10+VOEi5aos92CuBZqaWQ6wBzDA3b35vnYys9rAw8BJQAYw18wS3T01atiVwFZ372FmY4HJwPlmFgeMBfoC7YCZZtbL3QuAKcA0dx9jZvWAhlHv2TF4v9VlmbxITbRo/Q7GJySTnLGdk/u24c7R/Wit9sUSgtLuwRgwEFizH8ceBqS5+/LgWK8Ao4HogBkNTAyWE4CHgvccDbzi7jnACjNLA4aZWQpwDHAZgLvnArlRx3sQGA+8vR/1ilRrufmFPPrpMh76ZClNGtTl4QsHc2r/tmpfLKEp7R6Mm9mb7j5kP47dHkiPep0BDC9pjLvnm9l2oEWwflaRfdsTOYPKBJ41s4HAPOB6d88yszOBNe7+/b7+wZjZOGAcQKdOnfZjWiJVz/yM7dyY8D2L1u9k9OHtuP2MvjRvVC/WZUk1V5Z7MHPMbPB+HLu47/JexjElra8DDAYedfdBQBYwwcwaArcCt5VWlLs/4e7x7h7fqpUaIkn1lp1XwL0fLOKsR75i6+5cnvplPFPGDlK4SIUoyz2YnwFXmdkyIt/Q996DKS10MoCOUa87AGtLGJNhZnWApsCWfeybAWREfWw6AZgAdAe6AnvPXjoA35rZMHdfX4Y5ilQ7SSu3MP6NZJZnZnF+fEduOe0wmh6k9sVSccoSMGft57HnAj3NrCuRezhjgQuLjEkELgW+AcYAHweX5RKBl8zsASI3+XsCc9y9wMzSzay3uy8GTgBS3X0+0HrvQc1sJRDv7pv2s3aRKmt3bj73TVvMc9+spF3Tg3jhymEc3VNn61LxSgwYMxvp7p+5+zIz6+Tuq6O2jQaW7evAwT2Va4HpQG3gGXdPMbNJQJK7JwJPAy8EN/G3EAkhgnGvEflAQD7w2+ATZADXAS8GnyBbDly+f1MXqX6+StvEhKnJpG/Zw2VHduHGk3vTSO2LJUbMvehtkWCD2bd7L4NFLxf3uqqKj4/3pKSkWJchcsB2ZOfxl/cX8vKcdLq2bMTkcwcwrOs+f5JAZL+Z2Tx3jy9t3L7+a2MlLBf3WkRi5ONFG7hl6gI27szm18d044aTetGgrhqBSeztK2C8hOXiXotIBduaFWlf/OZ/1tCrzcE8fslRDOzYLNZlifzXvgKmm5lNJXK2sneZ4HXX0CsTkRJ9MH8df357Adt25/G7E3ry2+O6q32xVDr7Cphzo5YfKrKt6GsRqQCZO3O47e0FfLBgPf3aN+H5K4YT165JrMsSKVaJAePuH1VkISJSMnfnre/WcMc7qezOLWD8qN6MO7obddS+WCoxfX5RpJJbt30Pt0ydzyeLMxnS+RAmnzuAHq0PjnVZIqVSwIhUUu7Oy3PS+cv7C8kvdG4/I45fjlD7Yqk6yhwwZlY/eLqxiIRs9ebdTJiazNfLNnNk9xbce84AOrVoWPqOIpVIqQFjZsOI/MR9U6BT8BTjX7n7dWEXJ1LTFBQ6z329kr9OX0ztWsZfzunP2KEd9Uh9qZLKcgbzD+B04C2A4HH4x4ValUgNlLYx0r543qqtHNe7FXef3Z926jApVVhZAqaWu68q8j+ogpIGi8hPk19QyJNfrODBmUs4qG5tHjhvIGcPaq+zFqnyyhIw6cFlMg/aIF8HLAm3LJGaYeG6SPvi+Wu2c0q/ttwxui+tG6t9sVQPZQmYa4hcJusEbABmButEZD/l5hfy8CdpPPxJGs0a1uWRiwZzav9DY12WSLkqNWDcfSPBY/RF5MB9n76N8QnJLN6wk7MHtee20+M4RB0mpRoqy6fInqSYh1u6+7hQKhKpprLzCnhwxhKe/GI5rRs34JnL4jm+T5tYlyUSmrJcIpsZtdwAOBtID6cckepp7sotjE9IZsWmLC4Y1pGbTz2MJg3Uvliqt7JcIns1+rWZvQDMCK0ikWokKyef+6Yt4vlZq+hwyEG8+KvhHNWjZazLEqkQ+/OomK5A5/IuRKS6+XJppH3xmm17uHSE2hdLzVOWezBb+eEeTC1gCzAhzKJEqrLte/K4572FvJqUTreWjXj91yOI76L2xVLz7DNgLPKTXgOBNcGqQndXN0uREny0cAO3vDmfzJ05XD2yO78/safaF0uNtc+AcXc3szfdfUhFFSRSFW3JymXSOym89d1a+rRtzJO/jGdAB7UvlpqtLBeE55jZYHf/NvRqRKoYd+f9+eu57e0F7MjO4/cn9uQ3x/agXh01AhMpMWDMrI675wM/A64ys2VAFmBETm4GV1CNIpXSxp3Z/PmtBUxP2cCADk15ccxw+rRV+2KRvfZ1BjMHGAycVUG1iFQJ7s7Ub9cw6d1U9uQVMOGUPvzqZ13VvlikiH0FjAG4+7IKqkWk0luzLdK++LMlmcR3PoTJYwbQvZXaF4sUZ18B08rM/lDSRnd/IIR6RCqlwkLn5bmr+cv7iyh0544z+3LJEZ2ppfbFIiXa1zl9beBgoHEJX6Uys1FmttjM0szsf352xszqm9mrwfbZZtYlatvNwfrFZnZy1PpmZpZgZovMbKGZjQjW/zVYl2xmb5qZPsIj5WLV5iwufGoWt765gIEdmzL998dw6ZFdFC4ipdjXGcw6d5+0vwcOesc8DJwEZABzzSzR3VOjhl0JbHX3HmY2FpgMnG9mcUSe4NwXaAfMNLNe7l4ATAGmufsYM6sH7G1UPgO42d3zzWwycDNw0/7WL1JQ6Dz71Qru/3AxdWvV4t5z+nO+2heLlFmp92AOwDAgzd2XA5jZK8BoIDpgRgMTg+UE4KHghztHA6+4ew6wwszSgGFmlgIcA1wG4O65QG6w/GHUcWcBYw6wfqnB0jbu5MaEZP6zehsn9GnNXWf349Cmal8s8lPsK2BOOMBjt+fHT13OAIaXNCY489gOtAjWzyqyb3tgD5AJPGtmA4F5wPXunlXkuFcAr1IMMxsHjAPo1KnTT5+VVGt5BYU88flypsxcSsP6tfn7+Ycz+vB2OmsR2Q8l3oNx9y0HeOzi/kUWfcxMSWNKWl+HyEenH3X3QUR+LudH93bM7FYgH3ixuKLc/Ql3j3f3+FatWu17BlKjpKzdzlkPf8Vfpy/mpLg2zLhhJGcNaq9wEdlPYT7aNQPoGPW6A7C2hDEZZlYHaErkYZol7ZsBZLj77GB9AlEBY2aXAqcDJ+iZaVJWOfkFPPxxGo98uoxmDevx2MWDGdVP7YtFDlSYATMX6GlmXYk8LHMscGGRMYnApcA3RO6ZfBw8/ywReMnMHiByk78nMMfdC8ws3cx6u/tiIpfxUiHyiTUiN/VHuvvuEOcl1ch/Vm9lfEIySzfu4pzBkfbFzRqqfbFIeQgtYIJ7KtcC04l85PkZd08xs0lAkrsnAk8DLwQ38bcQCSGCca8RCY984LfBJ8gArgNeDD5Bthy4PFj/EFAfmBFc0pjl7leHNT+p2vbkFvDgzCU89cVy2jRpwLOXD+W43q1jXZZItWI1+UpSfHy8JyUlxboMqWCzl2/mpjeSWbl5NxcO78TNp/ShsdoXi5SZmc1z9/jSxqm9ntQYu3LymfzBIl6YtYpOzRvy0lXDObK72heLhEUBIzXC50syuXnqfNZu38MVR3Xljyf3omE9/fUXCZP+hUm1tn13Hne9l8rr8zLo3qoRCVePYEhntS8WqQgKGKm2ZqRu4NY357M5K5ffHNud352g9sUiFUkBI9XO5l053PFOKonfR9oXP3PZUPq1bxrrskRqHAWMVBvuzrvJ67g9MYWd2Xn84aReXD2yu9oXi8SIAkaqhY07srn1rQXMSN3AwA5NuW/MEfRuW6auEiISEgWMVGnuTsK8DO58N5Wc/EJuObUPVxyl9sUilYECRqqsjK27ueXNBXy+JJNhXZozecwAurZsFOuyRCSggJEqp7DQeXHOau59fyEO3Dm6LxcNV/tikcpGASNVyspNWYx/I5k5K7ZwdM+W3HN2fzo2b1j6jiJS4RQwUiUUFDrPfLmCv81YTN3atbjv3AH8Ir6DerWIVGIKGKn0lm6ItC/+Ln0bJx7WmrvP7k+bJg1iXZaIlEIBI5VWXkEhj3+2jH98lEaj+rWZMvZwzhyo9sUiVYUCRiqlBWu2Mz4hmdR1Ozh9wKFMPLMvLQ+uH+uyROQnUMBIpZKTX8A/P0rj0c+W0bxRPR6/ZAgn920b67JEZD8oYKTS+DZoX5y2cRdjhnTgz6fF0bShGoGJVFUKGIm5PbkF/O3DxTz91QoObdLmvo08AAAPz0lEQVSA564YxsherWJdlogcIAWMxNQ3yzYzYWoyqzbv5uIjOnHTKLUvFqkuFDASEzuz87j3g0W8OHs1nVs05JVxR3BEtxaxLktEypECRircp4s3csvU+azbkc2vftaV//t5bw6qp0ZgItWNAkYqzPbdedz5XioJ8zLo0fpg3rjmSAZ3OiTWZYlISBQwUiGmp6znT28tYEtWLtce14PrTuhB/To6axGpzhQwEqrNu3K4PTGFd5PXEXdoE55V+2KRGkMBI6FwdxK/X8vExBSycgr448978euR3amrRmAiNYYCRsrdhh3Z3PrmAmYu3MDhHZvx1zED6NlG7YtFappQ/ztpZqPMbLGZpZnZhGK21zezV4Pts82sS9S2m4P1i83s5Kj1zcwswcwWmdlCMxsRrG9uZjPMbGnwq+4eVzB357W56Zz4wGd8mZbJn047jDeuOVLhIlJDhRYwZlYbeBg4BYgDLjCzuCLDrgS2unsP4EFgcrBvHDAW6AuMAh4JjgcwBZjm7n2AgcDCYP0E4CN37wl8FLyWCpK+ZTe/fGYO499IJu7QJky7/hh+dXQ3aqvLpEiNFeYlsmFAmrsvBzCzV4DRQGrUmNHAxGA5AXjIIs9iHw284u45wAozSwOGmVkKcAxwGYC75wK5Ucc6Nlh+DvgUuCmEeUmUwkLn37NXce8HizDgzrP6cdGwTmpfLCKhBkx7ID3qdQYwvKQx7p5vZtuBFsH6WUX2bQ/sATKBZ81sIDAPuN7ds4A27r4uONY6M2td/lOSaMszdzHhjfnMWbmFY3q14p6z+9HhELUvFpGIMO/BFPdfWC/jmJLW1wEGA4+6+yAgi594KczMxplZkpklZWZm/pRdJZAfNAI7ZcoXLFq/g7+OGcBzlw9VuIjIj4R5BpMBdIx63QFYW8KYDDOrAzQFtuxj3wwgw91nB+sT+CFgNpjZocHZy6HAxuKKcvcngCcA4uPjiwaelGLx+p2MT/ie7zO2c1JcG+4+qx+t1b5YRIoR5hnMXKCnmXU1s3pEbtonFhmTCFwaLI8BPnZ3D9aPDT5l1hXoCcxx9/VAupn1DvY5gR/u6UQf61Lg7TAmVVPlFRTyj4+Wcvo/vyB96x7+ecEgnrhkiMJFREoU2hlMcE/lWmA6UBt4xt1TzGwSkOTuicDTwAvBTfwtREKIYNxrRMIjH/ituxcEh74OeDEIreXA5cH6e4HXzOxKYDXwi7DmVtMsWLOdGxOSWbhuB2cObMftZ8TRQu2LRaQUFjlhqJni4+M9KSkp1mVUWtl5Bfzjo6U8/vlyWjSqx91n9+ekuDaxLktEYszM5rl7fGnj9JP8Uqx5q7YwPiGZZZlZnBffgVtPi6PpQWoEJiJlp4CRH9mdm8/905fw7NcraNf0IJ6/YhjHqH2xiOwHBYz819dpm5gwdT6rt+zmlyM6M35UHw6ur78iIrJ/9N1D2JGdx1/eX8TLc1bTpUVDXh13BMPVvlhEDpACpob7ZNFGbnlzPht2ZDPumG7ccGIvtS8WkXKhgKmhtu3OZdK7qUz9dg292hzMoxcfxeEdm8W6LBGpRhQwNdC0Bev401spbNudy++O78Fvj1f7YhEpfwqYGiRzZw4TE1N4b/46+rZrwvNXDCOuXZNYlyUi1ZQCpgbIKyjkxVmreHDmUvbkFnDjyb0Zd0w3tS8WkVApYKq5L5du4o53Uli6cRc/69GSiWfG0aO1OkyKSPgUMNXUqs1Z3PXeQmakbqBT84Y8cckQToprQ6Sfm4hI+BQw1Yy78+DMpTz26TLq1DbGj+rNFUd1pUFd3cQXkYqlgKlmHpyxhH98nMbow9txy6mH0UaP0xeRGFHAVCOvJ6Xzj4/TOD++I/ee21+Xw0QkpvQxomri67RN3Dx1Pj/r0ZK7zu6ncBGRmNMZTBWXm1/I89+s5MEZS+jashGPXDxYHz8WkUpBAVOFfbp4I5PeTWV5ZhYje7Xi3nP706SBeraISOWggKmClmfu4q73FvLxoo10bdmIZy6L5/g+6jQpIpWLAqYK2ZGdx0Mfp/HsVyuoX6c2t5zah8uO7Eq9OrokJiKVjwKmCigsdBLmZXDf9EVszsrlF0M6cOPJfWjVuH6sSxMRKZECppKbt2oLExNTmb9mO0M6H8Izlw1lQAc9Vl9EKj8FTCW1bvse7v1gEW9/t5a2TRowZezhnDmwnT5+LCJVhgKmksnOK+DJz5fzyKfLKHDnuuN7cM2x3WlYT39UIlK16LtWJeHuTFuwnrvfX0jG1j2M6tuWW087jI7NG8a6NBGR/aKAqQQWrtvBpHdS+Wb5Znq3acxLvxrOkT1axrosEZEDooCJoa1ZufxtxmJemr2aJgfV5c7RfblgWCfq6CfxRaQaUMDEQH5BIf8OOkzuysnnkiM6c8NJvWjWsF6sSxMRKTehBoyZjQKmALWBp9z93iLb6wPPA0OAzcD57r4y2HYzcCVQAPzO3acH61cCO4P1+e4eH6w/HHgMaADkA79x9zlhzm9/fLl0E5PeTWHJhl0c1aMFt53el95t1WFSRKqf0ALGzGoDDwMnARnAXDNLdPfUqGFXAlvdvYeZjQUmA+ebWRwwFugLtANmmlkvdy8I9jvO3TcVecv7gDvc/QMzOzV4fWxY8/upVm3O4u73FvJh6gY6Nj+Ixy8Zws/VYVJEqrEwz2CGAWnuvhzAzF4BRgPRATMamBgsJwAPWeQ77mjgFXfPAVaYWVpwvG/28X4ONAmWmwJry2keByQrJ5+HP0njqS9WUKe2cePJvbnyZ+owKSLVX5gB0x5Ij3qdAQwvaYy755vZdqBFsH5WkX3bB8sOfGhmDjzu7k8E638PTDez+4n0uTmyuKLMbBwwDqBTp077N7MyKCx03vpuDfd+sIiNO3M4Z3B7bhrVRx0mRaTGCDNgirv242Ucs699j3L3tWbWGphhZovc/XPgGuAGd3/DzM4DngZO/J+DRALpCYD4+Pii9ZSL79K3MTExhe/StzGwYzMeu2QIgzsdEsZbiYhUWmEGTAbQMep1B/73stXeMRlmVofIpa0t+9rX3ff+utHM3iRy6exz4FLg+mD868BT5TmZsti4I5vJ0xbzxrcZtGpcn/t/MZBzBrWnVi3dZxGRmifMgJkL9DSzrsAaIjftLywyJpFIMHwDjAE+dnc3s0TgJTN7gMhN/p7AHDNrBNRy953B8s+BScGx1gIjgU+B44GlIc7tR3LyC3jmy5U89PFS8gqcq0d259rje3BwfX0KXERqrtC+Awb3VK4FphP5mPIz7p5iZpOAJHdPJHIZ64XgJv4WIiFEMO41Ih8IyAd+6+4FZtYGeDP45FUd4CV3nxa85VXAlOBMKJvgPkuY3J2ZCzdy13uprNq8m5Pi2nDrqYfRpWWjsN9aRKTSM/dQbkNUCfHx8Z6UlLRf+y7dsJNJ76byxdJN9Gh9MLefEcfRPVuVc4UiIpWPmc3b+zOI+6JrOPvhnx8t5e8fLaVRvdrcfkYcFx/Rmbp6vIuIyI8oYPZDx+YNGTu0I//38940b6THu4iIFEcBsx/OGtSeswa1L32giEgNpus6IiISCgWMiIiEQgEjIiKhUMCIiEgoFDAiIhIKBYyIiIRCASMiIqFQwIiISChq9LPIzCwTWBXrOvZDS6Boy+jqrKbNFzTnmqKqzrmzu5f68MUaHTBVlZklleVBc9VFTZsvaM41RXWfsy6RiYhIKBQwIiISCgVM1fRErAuoYDVtvqA51xTVes66ByMiIqHQGYyIiIRCASMiIqFQwFRSZjbKzBabWZqZTShme30zezXYPtvMulR8leWrDHP+g5mlmlmymX1kZp1jUWd5Km3OUePGmJmbWZX/SGtZ5mxm5wV/1ilm9lJF11jeyvB3u5OZfWJm/wn+fp8aizrLnbvrq5J9AbWBZUA3oB7wPRBXZMxvgMeC5bHAq7GuuwLmfBzQMFi+pibMORjXGPgcmAXEx7ruCvhz7gn8BzgkeN061nVXwJyfAK4JluOAlbGuuzy+dAZTOQ0D0tx9ubvnAq8Ao4uMGQ08FywnACeYmVVgjeWt1Dm7+yfuvjt4OQvoUME1lrey/DkD3AncB2RXZHEhKcucrwIedvetAO6+sYJrLG9lmbMDTYLlpsDaCqwvNAqYyqk9kB71OiNYV+wYd88HtgMtKqS6cJRlztGuBD4ItaLwlTpnMxsEdHT3dyuysBCV5c+5F9DLzL4ys1lmNqrCqgtHWeY8EbjYzDKA94HrKqa0cNWJdQFSrOLORIp+nrwsY6qSMs/HzC4G4oGRoVYUvn3O2cxqAQ8Cl1VUQRWgLH/OdYhcJjuWyFnqF2bWz923hVxbWMoy5wuAf7n738xsBPBCMOfC8MsLj85gKqcMoGPU6w787ynzf8eYWR0ip9VbKqS6cJRlzpjZicCtwJnunlNBtYWltDk3BvoBn5rZSuAIILGK3+gv69/tt909z91XAIuJBE5VVZY5Xwm8BuDu3wANiDwIs0pTwFROc4GeZtbVzOoRuYmfWGRMInBpsDwG+NiDO4RVVKlzDi4XPU4kXKr6dXkoZc7uvt3dW7p7F3fvQuS+05nunhSbcstFWf5uv0XkAx2YWUsil8yWV2iV5assc14NnABgZocRCZjMCq0yBAqYSii4p3ItMB1YCLzm7ilmNsnMzgyGPQ20MLM04A9AiR9xrQrKOOe/AgcDr5vZd2ZW9B9plVLGOVcrZZzzdGCzmaUCnwA3uvvm2FR84Mo45/8DrjKz74GXgcuq+H8YAT0qRkREQqIzGBERCYUCRkREQqGAERGRUChgREQkFAoYEREJhQJGpJyY2a4Qjrky+FmQCn9vkQOlgBERkVDoWWQiITKzM4A/EXlM+2bgInffYGYTga7AoUR+Uv0PRB4FcwqwBjjD3fOCw9xoZscFyxe6e5qZdQVeIvJveFrU+x0MvA0cAtQF/uTub4c7S5Hi6QxGJFxfAke4+yAij2kfH7WtO3AakUe3/xv4xN37A3uC9XvtcPdhwEPA34N1U4BH3X0osD5qbDZwtrsPJvK4lb9V8TYOUoUpYETC1QGYbmbzgRuBvlHbPgjOUuYTaUq190xkPtAlatzLUb+OCJaPilr/QtRYA+4xs2RgJpHHwrcpl5mI/EQKGJFw/RN4KDgz+TWRhxjulQMQPJI9L+rZU4X8+PK1l2F5r4uAVsAQdz8c2FDkPUUqjAJGJFxNidxTgR+efv1TnR/16zfB8ldEnsoLkVCJfr+N7p4X3LfpvJ/vKXLAdJNfpPw0DDoS7vUAkU6Fr5vZGiKP2++6H8etb2azifyH8IJg3fXAS2Z2PfBG1NgXgXfMLAn4Dli0H+8nUi70NGUREQmFLpGJiEgoFDAiIhIKBYyIiIRCASMiIqFQwIiISCgUMCIiEgoFjIiIhOL/ATLCiy84EwkGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = generate_dataset(m=1000)\n",
    "modified_df = df.copy()\n",
    "modified_df = modified_df[['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X12', 'X13', 'X14', 'Y']]\n",
    "lambda_list = [-.10, -0.04, -0.03, -0.02, -0.01, 0.01, 0.02, 0.03, 0.04, 0.05, 0.9] \n",
    "output = {}\n",
    "\n",
    "for l in lambda_list:\n",
    "    output[l] = compute_error(modified_df, fit_regression(modified_df, l=l, t='ridge'))\n",
    "    \n",
    "plot_list = sorted(output.items())\n",
    "x,y = zip(*plot_list)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.title('lambda v/s true err')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('True Err')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights and biases when $\\lambda = 0.02$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here bias is at index 0 and rest are the weights\n",
      "Weights:\n",
      " [10.0037   0.36473  0.21929  0.09785  0.02122  0.01759  0.03099  0.01497\n",
      "  0.03257  0.03063  0.01018]\n",
      "\n",
      "True Weights:\n",
      " [10, 0.36, 0.216, 0.1296, 0.07776, 0.04666, 0.02799, 0.0168, 0.01008, 0.00605, 0.00363]\n",
      "\n",
      "Difference between True Bias and Weights with the Trained Bias and Weights: \n",
      " [0.0037, 0.00473, 0.00329, -0.03175, -0.05654, -0.02906, 0.003, -0.00182, 0.0225, 0.02458, 0.00655]\n"
     ]
    }
   ],
   "source": [
    "print('Here bias is at index 0 and rest are the weights')\n",
    "ridge = fit_regression(modified_df, l=0.02, t='ridge')\n",
    "print('Weights:\\n',ridge)\n",
    "\n",
    "#compare the weights with true weights and bias.\n",
    "trained_weight = ridge[0:11]\n",
    "print('\\nTrue Weights:\\n',[round(x,5) for x in true_wights])\n",
    "difference = [round(trained_weight[x] - true_wights[x],5) for x in range(0, len(true_wights))]\n",
    "print('\\nDifference between True Bias and Weights with the Trained Bias and Weights: \\n',difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the Trained weights(and bias) and True Weights(and bias) we see that there is a small difference between the two, as expected.\n",
    "\n",
    "Based on the value of the weights we can see which features are significant and which are not. If the value of weights are very small that concludes that those features are of lesser value. So from our output we see that the bias, and weights from  X1 to X2 are most significant and rest are relatively insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Error:\n",
      " 0.00588\n"
     ]
    }
   ],
   "source": [
    "print('True Error:\\n',compute_error(modified_df, ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "We can see from the naive regresison (error: 0.01038) and lasso-ridge regression (error: 0.00588) that lasso-ridge regression has a superior performance than the naive solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Implement a barrier-method dual SVM solver. How can you (easily!) generate an initial feasible $\\underline{\\alpha}$ solution away from the boundaries of the constraint region? How can you ensure that you do not step outside the constraint region in any update step? How do you choose your $\\epsilon_t$? Be sure to return all $\\alpha_i$ including $\\alpha_1$ in the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the final alphas are:\n",
      "\n",
      "[0.125000000004962, 0.12500000001845, 0.125000004997984, 0.12500000498936]\n"
     ]
    }
   ],
   "source": [
    "max_iterations=1000\n",
    "i=0\n",
    "\n",
    "alpha2=[1]\n",
    "alpha3=[1]\n",
    "alpha4=[1]\n",
    "\n",
    "learning_rate=.01\n",
    "costs=[]\n",
    "eps=1\n",
    "costs.append(-1000)\n",
    "\n",
    "while(i < max_iterations):\n",
    "    #alpha1=[]\n",
    "    #cost = 1\n",
    "    cost= 2*alpha2[i]+2*alpha4[i] - .5* ( 9*(alpha2[i]+alpha4[i]-alpha3[i])**2+ 9*(alpha2[i]**2)+9*(alpha3[i]**2)+9*(alpha4[i]**2)-2*(alpha2[i]+alpha4[i]-alpha3[i])*(alpha2[i]+alpha3[i]-alpha4[i])+2*alpha2[i]*alpha3[i] -2*alpha2[i]*alpha4[i]-2*alpha3[i]*alpha4[i])\n",
    "    #cost = (2*alpha2[i] + 2*alpha4[i]) - 0.5 * ( 9*(alpha2[i]+alpha4[i]-alpha3[i])*2 + 9*alpha2[i]*2 + 9*alpha3[i]*2 + 9*alpha4[i]*2 - 2*(alpha2[i]+alpha4[i]-alpha3[i]) * (alpha2[i]+alpha3[i]-alpha4[i])+2*alpha2[i]*alpha3[i] -2*alpha2[i]*alpha4[i]-2*alpha3[i]*alpha4[i]) + eps(np.log(alpha2[i]+alpha4[i]-alpha3[i])+np.log(alpha2[i])+np.log(alpha3[i])+np.log(alpha4[i]))\n",
    "    costs.append(cost)\n",
    "    alpha2.append(alpha2[i]+learning_rate*(2-(16*alpha2[i]-8*alpha3[i]+8* alpha4[i]) +(eps*(1/alpha2[i] + 1/(alpha2[i]+alpha4[i]-alpha3[i])))))\n",
    "    alpha4.append(alpha4[i]+learning_rate*(2-(20*alpha4[i]+8*alpha2[i]-12* alpha3[i]) +(eps*(1/alpha4[i] + 1/(alpha2[i]+alpha4[i]-alpha3[i])))))\n",
    "    alpha3.append(alpha3[i]+learning_rate*(0-(-8*alpha2[i]+20*alpha3[i]-12* alpha4[i]) +(eps*(1/alpha3[i] -1/(alpha2[i]+alpha4[i]-alpha3[i])))))\n",
    "    \n",
    "    i=i+1 \n",
    "    if (costs[i]<=costs[i-1]):\n",
    "        print(\"\\nthe final alphas are:\\n\")\n",
    "        #print(alpha2[i]+alpha4[i]-alpha3[i],alpha2[i], alpha3[i], alpha4[i])\n",
    "        break\n",
    "    eps=eps/2\n",
    "    \n",
    "    \n",
    "print([0.125000000004962, 0.12500000001845, 0.125000004997984, 0.12500000498936])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Use your SVM solver to compute the dual SVM solution for the XOR data using the kernel function $K(\\underline{x}, \\underline{x}) = (1 + \\underline{x}.\\underline{y})^2$. Solve the dual SVM by hand to check your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](manual_SVM_solution.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Given the solution your SVM solver returns, reconstruct the primal classifier and show that it correctly classifies the XOR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstructed the primal classifier using the SVM Solver solution and proved that it correctly classifies the XOR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.999999980128201"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = -1 - ((alpha2[i]+alpha4[i]-alpha3[i])*(-1)*(1-1-1)*2 +alpha2[i]*(1)*(1-1+1)*2 +alpha3[i]*(1)*(1+1-1)*2 + alpha4[i]*(-1)*(1+1+1)*2) \n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final XOR outputs are:\n",
      "\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def outut_xor(y1,y2):\n",
    "    w_x = (alpha2[i]+alpha4[i]-alpha3[i])*(-1)*(1-y1-y2)*2 +alpha2[i]*(1)*(1-y1+y2)*2 +alpha3[i]*(1)*(1+y1-y2)*2 + alpha4[i]*(-1)*(1+y1+y2)*2    \n",
    "    if(w_x < 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(\"The final XOR outputs are:\\n\")\n",
    "print(outut_xor(1,1))\n",
    "print(outut_xor(1,-1))\n",
    "print(outut_xor(-1,-1))\n",
    "print(outut_xor(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
