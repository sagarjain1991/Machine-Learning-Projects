{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Assignment\n",
    "by Sagar Jain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Consider regression in one dimension, with a data set {(xi, yi)}i=1,...,m.\n",
    "Find a linear model that minimizes the training error, i.e., \\begin{equation*} \\dot{w} \\quad \\textrm{and} \\quad \\dot{b} \\end{equation*} to minimize\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=1}^m (\\dot{w}x_i +\\dot{b} - y_i)^2\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can write the above equation as\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=1}^m ( y_i-(\\dot{w}x_i +\\dot{b}))^2\n",
    "\\end{equation*}\n",
    "\n",
    "Lets expand this expression, we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "SquaredError_. line = ( y_1-(\\dot{w}x_1 +\\dot{b}))^2 + ( y_2-(\\dot{w}x_2 +\\dot{b}))^2 + ( y_3-(\\dot{w}x_3 +\\dot{b}))^2 + ...... + ( y_m-(\\dot{w}x_m +\\dot{b}))^2\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "=  \\quad  \\quad y_1^2 - 2y_1(\\dot{w}x_1 +\\dot{b}) + (\\dot{w}x_1 +\\dot{b})^2\n",
    "\\\\\\quad  \\quad + y_2^2 - 2y_2(\\dot{w}x_2 +\\dot{b}) + (\\dot{w}x_2 +\\dot{b})^2\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\\\quad  \\quad +y_m^2 - 2y_m(\\dot{w}x_m +\\dot{b}) + (\\dot{w}x_m +\\dot{b})^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "=  \\quad  \\quad y_1^2 - 2y_1\\dot{w}x_1 -2y_1\\dot{b} + \\dot{w}^2x_1^2 +2\\dot{w}x_1\\dot{b} +\\dot{b}^2\n",
    "\\\\\\quad  \\quad + y_2^2 - 2y_2\\dot{w}x_2 -2y_2\\dot{b} + \\dot{w}^2x_2^2 +2\\dot{w}x_2\\dot{b} +\\dot{b}^2\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\.\n",
    "\\\\\\quad  \\quad + y_m^2 - 2y_m\\dot{w}x_m -2y_m\\dot{b} + \\dot{w}^2x_m^2 +2\\dot{w}x_m\\dot{b} +\\dot{b}^2\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "=  \\quad  \\quad (y_1^2 + y_2^2 + y_3^2 + ..... + y_m^2) - 2w(y_1x_1 + y_2x_2 +.......+y_mx_m) -2b(y_1+y_2+...+y_m) + w^2(x_1^2+ x_2^2+....+x_m^2) +2wb(x_1+x_2+.....+x_m)+ mb^2\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "=  \\quad  \\quad m\\bar{y^2} -2wm\\bar{xy} -2bm\\bar{y} + w^2m\\bar{x^2} + 2wbm\\bar{x} +mb^2\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Now inorder to minimise the  \\begin{equation*} \\dot{w} \\quad and  \\quad  \\dot{b}\\end{equation*} \n",
    "\n",
    "we need to differentiate the above equation w.r.t to w and b and equate them to zero. that is \n",
    "\n",
    "\\begin{equation*} \\frac{\\partial SE}{\\partial w} = 0 \\quad and  \\quad  \\frac{\\partial SE}{\\partial b} = 0\n",
    "\\end{equation*}\n",
    "\n",
    "Differentiating the squarederror equation, we get\n",
    "\n",
    "\\begin{equation*} \n",
    "-2m\\bar{xy} + 2m\\bar{x^2}w + 2bm\\bar{x}= 0 \\quad and  \\quad  -2m\\bar{y} + 2mw\\bar{x} + 2bm= 0\n",
    "\\\\= -\\bar{xy} + w\\bar{x^2} + b\\bar{x}= 0 \\quad and  \\quad  -\\bar{y} + w\\bar{x} + b= 0\n",
    "\\\\\n",
    "\\\\\n",
    "\\\\ w\\bar{x^2} + b\\bar{x} = \\bar{xy}  \\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad ..........(1)\n",
    "\\\\ and  \\quad   w\\bar{x} + b= \\bar{y} \\quad \\quad \\quad \\quad \\quad \\quad\\quad\\quad ..........(2)\n",
    "\\end{equation*}\n",
    "\n",
    "By looking at the above equation, we can conclude that point \\begin{equation*} (\\bar{x},\\bar{y})  \\quad and \n",
    " \\quad (\\frac{\\bar{x}^2}{\\bar{x}},\\frac{\\bar{xy}}{\\bar{x}})  \\quad lies \\quad on \\quad the \\quad best \\quad line  \\quad y = wx+b \\end{equation*}\n",
    "\n",
    "Solving equation 1 and 2, we get the values of w and b which will minimise the mean squared error.\n",
    "\n",
    "\\begin{equation*} \n",
    "\\hat{w} = \\frac{\\bar{x}\\bar{y}-\\bar{xy}}{\\bar{x}^2-\\bar{x^2}} \\quad and \\quad \\hat{b} = y- (\\frac{\\bar{x}\\bar{y}-\\bar{xy}}{\\bar{x}^2-\\bar{x^2}})\\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Assume there is some true linear model, such that \\begin{equation*} y_i = wx_i + b + \\epsilon \\end{equation*}\n",
    ", where noise variables E are i.i.d.\n",
    "\n",
    "with  \\begin{equation*}\\epsilon ∼ N(0, σ^2)\\end{equation*}. Argue that the estimators are unbiased, i.e., \n",
    "\\mathbb{E}[\\hat{w}]$=w and $\\mathbb{E}[\\hat{b}]$=b\n",
    "\n",
    "What are the variances of these estimators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  $\\hat{w}$ can be written as $\\frac{c_{XY}}{s^2_x} $\n",
    "where $s^2_x = \\bar{x}^2-\\bar{x^2}$ and $c_{XY} = \\bar{x}\\bar{y}-\\bar{xy}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll start with the slope, $\\hat{w}$\n",
    " \\begin{equation*}\n",
    "\\hat{w} = \\frac{c_{XY}}{s^2_x}\n",
    "\\\\  = \\frac{\\frac{1}{m}\\sum_{i=1}^mx_i(y_i-\\bar{x}\\bar{y}}{s^2_x}\n",
    "\\\\ = \\frac{\\frac{1}{m}\\sum_{i=1}^mx_i(b+w_ix_i+\\epsilon_i)-\\bar{x}(b+w_i\\bar{x}+\\bar{\\epsilon})}{s^2_x}\n",
    "\\\\ = \\frac{b\\bar{x}+w\\bar{x^2} + \\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i -\\bar{x}b-w\\bar{x^2}-\\bar{x}\\bar{\\epsilon} }{s^2_x}\n",
    "\\\\ = \\frac{ws^2_x+\\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i-\\bar{x}\\bar{\\epsilon}}{s^2_x}\n",
    "\\\\ = w+\\frac{\\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i -\\bar{x}b-w\\bar{x^2}-\\bar{x}\\bar{\\epsilon}}{s^2_x}\n",
    " \\end{equation*}\n",
    " \n",
    " since $\\bar{x}\\bar{\\epsilon} = n^{-1}\\sum_{i}\\bar{x}\\epsilon_i$\n",
    " \\begin{equation*}\n",
    " \\\\\n",
    " \\hat{w} = w+\\frac{\\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i-\\bar{x}\\bar{\\epsilon}}{s^2_x}\n",
    "  \\end{equation*}\n",
    "  \n",
    "This representation of the slope estimate shows that it is equal to the trueb slope (w) plus something which depends on the noise terms (the $\\epsilon_i$, and their sample average $\\bar{\\epsilon}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected value and bias:\n",
    "\n",
    "Recall that $\\mathbb{E}[\\epsilon_i|X_i]$=0 , so\n",
    "\\begin{equation*}\n",
    "\\frac{1}{m}\\sum_{i=1}^m(x_i-\\bar{x})\\mathbb{E}[\\epsilon_i] = 0\n",
    "\\end{equation*}\n",
    "\n",
    "Thus,\n",
    "$\\mathbb{E}[\\hat{w}] = w$\n",
    "\n",
    "Since the bias of an estimator is the difference between its expected value\n",
    "and the truth, $\\hat{w}$ is an unbiased estimator of the optimal slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning to the intercept,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{E}[\\hat{b}] = \\mathbb{E}[\\bar{Y}-\\hat{w}\\bar{X}]\n",
    "\\\\\n",
    "= b + w\\bar{X} - \\mathbb{E}[\\hat{w}]\\bar{X}\n",
    "\\\\\n",
    "= b+w\\bar{X}-w\\bar{X}\n",
    "\\\\ \n",
    "=b\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so it is also unbiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance and Standard Error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{w}] = \\mathbb{Var}[w+\\frac{\\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i-\\bar{x}\\bar{\\epsilon}}{s^2_x}]\n",
    "\\\\\n",
    "= \\mathbb{Var}[\\frac{\\frac{1}{m}\\sum_{i=1}^mx_i\\epsilon_i-\\bar{x}\\bar{\\epsilon}}{s^2_x}]\n",
    "\\\\\n",
    "=\\frac{\\frac{1}{n^2}\\sum_{i=1}^m(x_i-\\bar{x})^2\\mathbb{Var}[\\epsilon_i]}{(s^2_x)^2}\n",
    "\\\\\n",
    "=\\frac{\\frac{\\sigma^2}{m}s^2_x}{(s^2_x)^2}\n",
    "\\\\\n",
    "= \\frac{\\sigma^2}{ms^2_x}\n",
    "\\end{equation*}\n",
    "\n",
    "Hence, \n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{w}] \\quad is \\quad approximately \\quad equal \\quad to \\quad \\frac{\\sigma^2}{m\\mathbb{Var}(x)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this says that the variance of the slope estimate goes up as the noise around the regression line $\\sigma^2$ gets bigger, and goes down as we have more observations (m), which are further spread out along the horizontal axis\n",
    "$s^2_x$; it should not be surprising that it’s easier to work out the slope of a line from many, well-separated points on the line than from a few points smushed together\n",
    "\n",
    "Similarly for calculating the variance for $\\hat{b}$\n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{b}]  = \\mathbb{Var}[\\bar{y}]+\\bar{x}^2\\mathbb{Var}[\\hat{w}]-2\\bar{x}\\mathbb{Cov}(\\bar{y},w) \\quad \\quad\\quad\\quad .....eq(3)\n",
    "\\end{equation*}\n",
    "\n",
    "On calculating $\\mathbb{Cov}(\\bar{y},w)$ we get 0 value.\n",
    "\n",
    "Putting all the values in the above equation, we get \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{b}] = \\sigma^2(\\frac{1}{m}+\\frac{\\bar{x}^2}{(s^2_x)^2})\n",
    "\\end{equation*}\n",
    "Hence,\n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{b}] \\quad  is \\quad approximately\\quad  equal \\quad to \\quad \\frac{\\sigma^2\\mathbb{E}[x^2]}{m\\mathbb{Var}(x)}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4\n",
    "Argue that recentering the data $(x^`_i=x_i-µ)$ and doing regression on the re-centered data produces the\n",
    "same error on $\\hat{w}$ but minimizes the error on $\\hat{b}$  when $µ = \\mathbb{E}[x]$ (which we approximate with the sample mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated the variances of both estimators and we can observe that \n",
    "\n",
    "Variance of w is independent of the shift of data from one place to the origin as \n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{w}] = \\frac{\\sigma^2}{m\\mathbb{Var}(x)}\n",
    "\\end{equation*}\n",
    "in above equation Var(x) will not get changed even if we will shift the data from one point to another point.\n",
    "Therefore, we can conclude that doing regression on the re-centered data produces the\n",
    "same error on $\\hat{w}$.\n",
    "\n",
    "While in the case of variance of b, which is\n",
    "\\begin{equation*}\n",
    "\\mathbb{Var}[\\hat{b}] = \\frac{\\sigma^2\\mathbb{E}[x^2]}{m\\mathbb{Var}(x)}\n",
    "\\end{equation*}\n",
    "\n",
    "depends on the $\\mathbb{E}[x^2]$\n",
    "\n",
    "Therefore, we can conclude that doing regression on the re-centered data produces the minimise error on $\\hat{b}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Observation\n",
    "\n",
    "Also we can observe while calculating the values of w and b that,\n",
    "\n",
    "\n",
    "\\begin{equation*} \n",
    "\\hat{w} = \\frac{\\bar{x}\\bar{y}-\\bar{xy}}{\\bar{x}^2-\\bar{x^2}} \\quad and \\quad \\hat{b} = y- (\\frac{\\bar{x}\\bar{y}-\\bar{xy}}{\\bar{x}^2-\\bar{x^2}})\\bar{x}\n",
    "\\end{equation*}\n",
    "\n",
    "So when we look at the eqn. for $w$ we see that the absolute value of $x$ had less dependence on $w$, so if we shift the value of x the difference in the numerator and denomenator will change with same amount and when divided will provide the orignal fraction.\n",
    "\n",
    "Where as in the case of $b$, we see that its directly proportional to $-x$. Hence any shift in x will directly affect the value of $b$. Also it depends on the value of $\\bar{x}$. And the value of $\\bar{x}$ is minimum when the values of $x$ are taken around the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Verify this numerically in the following way: Taking $m = 200, w = 1, b = 5,  \\sigma^2 = 0.1$.\n",
    "- Repeatedly perform the following numerical experiment: generate $x_1, . . . , x_m $ ~ $ Unif(100, 102)$, $y_i = wx_i + b + \\epsilon_i$ (with $\\epsilon_i$ as a normal, mean 0, variance $\\sigma^2$), and $x_i' = x_i - 101$, compute  $\\hat{w}, \\hat{b}$ based on the $\\{(x_i, y_i)\\}$ data, and $\\hat{w}', \\hat{b}'$ based on the $\\{(x_i', y_i)\\}$ data.\n",
    "- Do this 1000 times, and estimate the expected value and variance of $\\hat{w}, \\hat{w}', \\hat{b}, \\hat{b}'$. Do these results make sense? Do these results agree with the above limiting expressions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing global variables\n",
    "m = 200\n",
    "w = 1\n",
    "b = 5\n",
    "\n",
    "mean = 0\n",
    "sigma_sq = 0.1\n",
    "\n",
    "#function to create dataset\n",
    "def create_dataset():\n",
    "    epsilon = np.random.normal(mean, np.sqrt(sigma_sq), 1)\n",
    "\n",
    "    x = np.random.uniform(low=100, high=102, size=m)\n",
    "    y = (w * x) + b + epsilon\n",
    "\n",
    "    x_dash = x - 101\n",
    "    \n",
    "    return x, y, x_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coefficients(x, y):\n",
    "    \n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)\n",
    "    \n",
    "    x_bar_y_bar = x_bar * y_bar\n",
    "    \n",
    "    xy = x * y\n",
    "    xy_bar = np.mean(xy)\n",
    "    \n",
    "    x_sq = x ** 2\n",
    "    x_sq_bar = np.mean(x_sq)\n",
    "    \n",
    "    x_bar_sq = x_bar ** 2\n",
    "    \n",
    "    w_hat = (x_bar_y_bar - xy_bar) / (x_bar_sq - x_sq_bar)\n",
    "    \n",
    "    b_hat = y_bar - (w_hat * x_bar)\n",
    "    \n",
    "    return w_hat, b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate the expriment 1000 times\n",
    "simulation_count = 1000\n",
    "\n",
    "w_hat = []\n",
    "w_hat_dash = []\n",
    "\n",
    "b_hat = []\n",
    "b_hat_dash = []\n",
    "\n",
    "for count in range(0, simulation_count):\n",
    "    x, y, x_dash = create_dataset()\n",
    "    \n",
    "    w, b = compute_coefficients(x, y)\n",
    "    w_hat.append(w)\n",
    "    b_hat.append(b)\n",
    "    \n",
    "    w_dash, b_dash = compute_coefficients(x_dash, y)\n",
    "    w_hat_dash.append(w_dash)\n",
    "    b_hat_dash.append(b_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of W_hat:  0.999999999908474\n",
      "Variance of W_hat:  5.400003811314988e-21\n"
     ]
    }
   ],
   "source": [
    "#Estimate the expected value and variance of w_hat\n",
    "print('Expected value of W_hat: ',np.mean(w_hat))\n",
    "print('Variance of W_hat: ',np.var(w_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of W_hat_dash:  0.9999999999086403\n",
      "Variance of W_hat_dash:  5.402766126968039e-21\n"
     ]
    }
   ],
   "source": [
    "#Estimate the expected value and variance of w_hat_dash\n",
    "print('Expected value of W_hat_dash: ',np.mean(w_hat_dash))\n",
    "print('Variance of W_hat_dash: ',np.var(w_hat_dash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of b_hat:  5.025041398157789\n",
      "Variance of b_hat:  5.223345644933421\n"
     ]
    }
   ],
   "source": [
    "#Estimate the expected value and variance of b_hat\n",
    "print('Expected value of b_hat: ',np.mean(b_hat))\n",
    "print('Variance of b_hat: ',np.var(b_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of b_hat_dash:  106.02504138891366\n",
      "Variance of b_hat_dash:  5.22334566551959\n"
     ]
    }
   ],
   "source": [
    "#Estimate the expected value and variance of b_hat_dash\n",
    "print('Expected value of b_hat_dash: ',np.mean(b_hat_dash))\n",
    "print('Variance of b_hat_dash: ',np.var(b_hat_dash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Yes, the results make sense. The value of $x$ is shifted to $x'$, as expected the value of intercept is also shifted by the same amount, as seen in above simulation. It does agree with the above limiting equation in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "Intuitively, why is there no change in the estimate of the slope when the data is shifted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "The linear model $y = wx + b$ gives a straight line which try to minimize the mean squared error (MSE) of distance between point and the line. When the data is shifted on the x-axis, its coordinates for y-axis remains the same. As the data points have not change their relative position with each other, a similar shifted line will provide a line which minimizes the MSE of the data. This shifted line thus has the same slope but a shifted intercept parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "Consider augmenting the data in the usual way, going from one dimensions to two dimensions, where the first coordinate of each $\\underline x$ is just a constant 1. Argue that taking $\\Sigma = X^{T}X$ in the usual way, we get in the limit that\n",
    "\n",
    "$$\n",
    "\\Sigma \\rightarrow m\n",
    "\\begin{bmatrix}\n",
    "    1 & \\mathbb{E}[x] \\\\\n",
    "    \\mathbb{E}[x] & \\mathbb{E}[x^2] \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Show that re-centering the data $(\\Sigma = (X')^{T}(X')$, taking $x_i' = x_i - \\mu)$, the condition number $\\kappa(\\Sigma')$ is minimized taking $\\mu = \\mathbb{E}[x].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "When we transform the data from 1-D to 2-D we consider the matrix of data X. So in this case taking the square of the initial matrix X, we get $\\Sigma = X^{T}X$.\n",
    "\n",
    "So when we compute individual value of x of the $\\Sigma$ matrix such as $x_{1,1}, x_{1,2} ... , x_{m,m}$\n",
    "\n",
    "For the diagonal values we get\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{m}\\Sigma_{1,1} = \\frac{1}{m}\\sum_{i=0}^mX_1^iX_1^i = \\mathbb{E}[X_1, X_1] = \\mathbb{E}[X_1^2] \\\\\n",
    "\\frac{1}{m}\\Sigma_{1,2} = \\frac{1}{m}\\sum_{i=0}^mX_1^iX_2^i = \\mathbb{E}[X_1, X_2] \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "So on computing for all values from 0 to m then we get $\\Sigma$ in the form,\n",
    "\n",
    "$$\n",
    "\\Sigma \\rightarrow m\n",
    "\\begin{bmatrix}\n",
    "    1 & \\mathbb{E}[x] \\\\\n",
    "    \\mathbb{E}[x] & \\mathbb{E}[x^2] \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recentering the data is usually done for the purpose of preconditioning the data.\n",
    "\n",
    "We considered the data points x1 = (100, 50), x2 = (100, 52), x3 = (101, 51). We calculated the largest eigenvalue of $X^TX$ which comes out to be 38, 000, and the smallest comes out to be 1.6, giving a condition number of\n",
    "κ(Σ) ≈ 22, 000.\n",
    "\n",
    "In this case, note that the average data point $\\bar{x}$ is (100.333, 51)\n",
    "If we will ‘center’ the data by subtracting off this mean, we get x1 = (−1/3, −1), x2 = (−1/3, 1), x3 = (2/3, 0).\n",
    "\n",
    "Building the data matrix out of this re-centered data we get that $(X^`)^TX^`$ has eigenvalues of 2 and 2/3, with a total condition number of κ(Σ0) = 3.\n",
    "\n",
    "This represents a massive improvement in the relative error in various directions, with nothing more complicated than re-centering the data to have mean 0. Re-centering like this ensures that the principal components of the data really capture the fundamental geometry of the data, rather than simply where the cloud is sitting in space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
